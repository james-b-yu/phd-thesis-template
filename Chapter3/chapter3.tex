%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Construction and Well-Posedness}\label{cha:3}

% **************************** Define Graphics Path **************************
\ifpdf
\graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
\graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

In \Cref{cha:2}, we introduced stochastic interpolants (SIs) in their original finite-dimensional setting, noting their advantages over diffusion models (DMs). % todo: elaborate
While DMs have been successfully generalised to achieve state-of-the-art results in function spaces, SIs have not yet been framed in function spaces. Furthermore, existing SI formulations are primarily generative; they do not explicitly guarantee that evolving a process from a point yields a sample from the true conditional target distribution. This conditional sampling capability is essential for the Bayesian inverse problems that are a central motivation for this thesis.

This chapter addresses both of these gaps. We develop a framework for stochastic interpolants on infinite-dimensional Hilbert spaces, explicitly addressing the cases of non-conditional and conditional conditional sampling. We will refer to the former as a \textit{marginal bridge} and the latter as an \textit{conditional bridge}.

For clarity of presentation, our formal analysis will focus on the process that evolves from the source to the target distribution. The corresponding results for the time-reversed evolution are analogous, and we detail this symmetry in \Cref{cha:3.backwards}. % TODO: signpost throughout this bridge

% TODO: add detail; signpost better
% maybe give summery of the chapter here, eg. we begin

\section{Framework}

% TODO: make sure you have a section or at least paragraph that explicitly addresses bayesian forward/inverse problems

Let \(H\) be a real, separable Hilbert space equipped with the inner product \(\ev{\cdot, \cdot}_{H}\) and let \(\mu\) be a Borel probability measure on the product space \(H \times H\). The marginals of \(\mu\), denoted by \(\mu_{0}\) and \(\mu_{1}\), are the pushforward measures under the canonical projection maps onto the first and second components of the project space, that is, \(\mu_{0}(\dd{\xi_{0}}) = \mu(\dd{\xi_{0}}\times H)\) and \(\mu_{1}(\dd{\xi_{1}}) = \mu(H \times \dd{\xi_{1}})\).

% TODO: all "laws" throughout the thesis should be replaced with "distribution"/measure

\begin{definition}\label{dfn:stochint}
  A \textit{stochastic interpolant} (SI) is a family of \(H\)-valued random variables \(\qty{x_{t}}_{t \in [0, 1]}\) indexed by time \(t \in [0, 1]\) such that
  \[
    x_{t} = \alpha(t) \xi_{0} + \beta(t) \xi_{1} + \gamma(t)z,
  \]
  where:
  \begin{enumerate}
    \item \(\alpha(t), \beta(t), \gamma(t) : [0, 1] \to \mathbb{R}_{\geq 0}\) defined such that \(\alpha\) and \(\beta\) are continuously differentiable on \([0, 1]\), and \(\gamma\)  is continuous on \([0, 1]\) and continuously differentiable on \((0, 1)\). They satisfy the boundary conditions \(\alpha(0) = \beta(1) = 1, \alpha(1) = \beta(0) = 0\), \(\gamma(0) = \gamma(1) = 1\), and \(\gamma(t) > 0\) for all \(t \in (0, 1)\).
    \item The pair of random variables \(\xi = (\xi_0, \xi_1)\) is drawn from the joint probability measure \(\mu\).
    \item The random variable \(z\) distributed independently of \(\xi\) and drawn from a Gaussian measure \(\operatorname{N}(0, C)\), where \(C : H\to H\) is a positive-definite trace-class covariance operator.
  \end{enumerate}
\end{definition}

Throughout, we denote \(\dot{x}_{t}\coloneqq \dot{\alpha}(t) \xi_{0} + \dot{\beta}(t) \xi_{1} + \dot{\gamma}(t) z\). We refer to the components of the data pair \(\xi = (\xi_{0}, \xi_{1}) \sim \mu\) as the \textit{source data} \(\xi_{0}\) and \textit{target data} \(\xi_{1}\), with corresponding \textit{source distribution} \(\mu_{0}\) and \textit{target distribution} \(\mu_{1}\). The joint measure \(\mu\) also induces a conditional distribution of the target given source data: for \(\mu_{0}\)-almost every \(x \in H\), we write \(\mu_{1 \mid 0}(\dd{x}, x_{0})\) to denote the  conditional distribution of \(\xi_{1}\) on \(H\), conditional on \(\xi_{0} = x\).

% TODO: signpost difficulty arising from gamma0 = 0?

\subsection{Marginal Bridge}
We first construct a stochastic process that bridges the source distribution \(\mu_{0}\), to the target distribution, \(\mu_{1}\). We refer to this process as the \textit{marginal bridge}, which distinguishes it from the \textit{conditional bridge} to be detailed in \Cref{ssec:condbridge}.

Using the same terminology as in \citet{albergo2023stochasticinterpolantsunifyingframework}, we define \textit{velocity} and \textit{denoiser} functions \(\zeta, \eta: [0, 1] \times H \to H\) to be the following conditional expectations. % TODO: is this good,
\begin{align}
  \zeta(t, x) &\coloneqq \mathop{\mathbb{E}}\qty[ \dot{x}_{t} \mid x_{t} = x], \label{eqn:zetadef} \\
  \eta(t, x) &\coloneqq \mathop{\mathbb{E}}\qty[z \mid x_{t} = x]. \label{eqn:etadef}
\end{align}

The marginal bridge is a stochastic process \(X_{t}\) governed by the following equation, which we call the MB-SDE:
\begin{equation}
  \dd{X_{t}} \coloneqq \qty(\zeta(t, X_{t}) - \frac{\varepsilon}{\gamma(t)} \eta(t, X_{t}))\dd{t} + \sqrt{2\varepsilon} \dd{W_{t}}, \quad X_{0} \sim \mu_{0}. \label{eqn:mbsde}
\end{equation}
where \(W_{t}\) is a \(C\)-Wiener process and \(\varepsilon \geq 0\) is a scalar. We use the following to denote the drift coefficient of the MB-SDE (\ref{eqn:mbsde}):
\begin{equation}
  f(t, x) \coloneqq \zeta(t, x) - \frac{\varepsilon}{\gamma(t)} \eta(t, x)  \label{eqn:deff} %TODO: acknowledge veps=0 as ode?
\end{equation}

Assuming that the MB-SDE (\ref{eqn:mbsde}) has any weak solution on a, possibly strict, subinterval \([0, \overline{t}] \subseteq [0, 1]\), standard results (see e.g., \citealp[][Chapter 14.2.2]{da2014stochastic}) show that for \(\dd{t}\)-almost every \(t \in [0, \overline{t}]\), the marginal distribution \(\rho_{t}\) of this solution at time \(t\) satisfies the following \textit{Fokker-Plank} equation:
\begin{equation}
  \dv{t} \int_{H} u(t, x) \rho_{t}(\dd{x}) = \int_{H} \mathcal{L} u(t, x) \rho_{t}(\dd{x}), \label{eqn:fp}
\end{equation}
for all test functions \(u(t, x)\) in the space \(E\) formed by the linear span of the real and imaginary components of functions of the form
\begin{equation}
  u_{\phi, h}(t, x) = \phi(t) e^{i \ev{x, h(t)}_{H}},  \text{ for any }\phi \in C^{1}([0, \overline{t}]), h \in C^{1}([0, \overline{t}]; H), \label{eqn:testfns}
\end{equation}
and where where \(\mathcal{L}\) is a \textit{Kolmogorov operator} given by:
\[
  \mathcal{L} u(t, x) \coloneqq  \operatorname{Tr}\qty(\varepsilon C D^{2}_{x} u(t, x)) + D_{t}u(t, x) + \ev{f(t, x), D_{x} u(t, x)}_{H}.
\]

We use \(D_{t}\) to denote the derivative in time, and \(D_{x}, D^{2}_{x}\) the first and second-order Frechet derivatives in Hilbert space. % TODO: define Frechet derivatives?

The Fokker-Planck equation (\ref{eqn:fp}) fundamentally describes the evolution of the probability distribution of a stochastic process. In finite dimensions, this is typically stated directly in terms of the density of the law of the solution at each time point, with respect to the Lebesgue measure. In contrast, in infinite dimensions a time-uniform reference measure is not guaranteed to exist and hence we instead state the Fokker-Plank equation in terms of test functions \(u(t, x)\).

To show that the MB-SDE (\ref{eqn:mbsde}) provides a valid path that correctly transports a source measure \(\mu_{0}\) to a target measure \(\mu_{1}\), we show that the marginal distribution \(\mu_{t}\) of our stochatic interpolant also satisfies \Cref{eqn:fp} on the entire time interval \(t \in [0, 1]\). Our main technical contribution is showing this relationship holds in infinite-dimensions via test functions, avoiding the need to express measures via densities.

\begin{restatable}{lemma}{restatelemfpmarg}\label{lem:fpmarg}
  Let \(\mu_{t}\) be the marginal distribution of the stochatic interpolant \(x_{t}\), defined in \Cref{dfn:stochint}. For every \(t \in [0, 1]\), the measure \(\mu_{t}\) satisfies the Fokker-Plank equation (\ref{eqn:fp}).
\end{restatable}
\begin{proof}[Proof (sketch)]
  The full proof is presented in \Cref{prf:lem:fpmarg} in Appendix \ref{app:A}. Our strategy is to consider the characteristic function of the real-valued random variable \(u(t, x_{t})\) to provide an expression for the time derivative of the expected value of \(u(t, x_{t})\), which is the left-hand side of \Cref{eqn:fp}. We apply the law of iterated expectations to express this in terms of the drift term \(f(t, x_{t})\). We then recover the trace term by applying Parseval's theorem and expressing inner products as an infinite sum of projections onto an eigenbasis of the covariance operator \(C\).
\end{proof}

Having established that both \(\rho_{t}\) and \(\mu_{t}\) satisfy the Fokker-Plank equation (\ref{eqn:fp}), we state our main result justifying the MB-SDE (\ref{eqn:mbsde}) as a suitable stochastic process allowing one to bridge \(\mu_{0}\) to \(\mu_{1}\).
\begin{restatable}{theorem}{restatethmmbsde}\label{thm:mbsde}
  Let \(\mu_{t}\) be the law of the stochastic interpolant \(x_{t}\) at time \(t\).
  \begin{enumerate}
    \item \label{ass:uniquelaw} Suppose that the MB-SDE (\ref{eqn:mbsde}) has solutions which are unique in law on a non-empty time interval \([0, \overline{t}] \subseteq [0, 1]\). We denote the law of \(X_{t}\) by \(\rho_{t}\).
    \item \label{ass:densespace} Suppose that \(\mathcal{L}E\) is dense in \(L^{1}([0, \overline{t} ]\times H, \nu)\), where \(\nu\) is the measure on \([0, \overline{t}] \times H\) determined uniquely by
      \[
        \nu(\dd{(t, x)}) = \nu_{t}(\dd{x}) \dd{t},
      \]
      and \(\nu_{t} \coloneqq \frac{1}{2} \rho_{t} + \frac{1}{2} \mu_{t}\) for each \(t \in [0, \overline{t}]\).

  \end{enumerate}

  Then, for \(\dd{t}\)-almost every \(t \in [0, \overline{t}]\), we have
  \[
    \rho_{t} = \mu_{t}.
  \]
\end{restatable}
\begin{proof}[Proof (sketch)]
  The full proof is presented in \Cref{prf:thm:mbsde} in Appendix \ref{app:A}. We follow a similar line of reasoning to \citet[][Theorem 2.1]{bogachev2010uniquenesssolutionsfokkerplanckequations}, who study the uniqueness of solutions to Fokker-Plank equations in infinite dimensions. By exploiting the deness of \(\mathcal{L}E\) in \(L^{1}([0, 1] \times H, \nu)\), we show for \(\dd{t}\)-almost every \(t\) that the signed measure \(\rho_{t} - \mu_{t}\) is zero, and hence \(\rho_{t} = \mu_{t}\).
\end{proof}

\Cref{thm:mbsde} means that the MB-SDE (\ref{eqn:mbsde}) successfully bridges from the source to the target distribution: starting with a sample from the source distribution, we can solve the MB-SDE (\ref{eqn:mbsde}) forward in time to obtain a samples from the source distribution \(\mu_{0}\) provided we can learn the drift coefficient \(f(t, x)\).

The validity of this result rests on two key assumptions. Our subsequent analysis in \Cref{TODO} addresses the first assumption, the existence of a unique weak solution, by proving a stronger result: the existence and uniqueness of a \textit{strong solution}. Strong uniqueness enables us to employ a coupling argument to bound the Wasserstein distance between our generated samples and the true target distribution (see TODO \ref{TODO}). % TODO: add the reference to the theorem

Our second assumption adopts the framework of \citet[][Theorem 2.1]{bogachev2010uniquenesssolutionsfokkerplanckequations}. The density condition on the Kolmogorov operator's range guarantees uniqueness for the Fokker-Planck equation. This technical requirement ensures the space of test functions is sufficiently rich to exclude spurious solutions to the Fokker-Planck equation beyond the one generated by the MB-SDE. While essential for our proof, a detailed analysis of the minimal requirements to ensure it holds is a distinct line of inquiry that we leave for future work.

% TODO: need to define what L1(..) means

Thus far, we have focused on the marginal bridge SDE, which provides a mechanism to sample from a target distribution \(\mu_{1}\). However,  to solve Bayesian forward and inverse problems we are required not to sample from a marginal, but from a conditional distribution. To address this, we now extend our framework to construct a conditional bridge SDE (CB-SDE). We detail this process in the following section.

\subsection{Conditional Bridge}\label{ssec:condbridge}
We now construct a stochastic process called the \textit{conditional bridge} which, conditional on a draw \(\xi_{0} \sim \mu_{0}\), forms a bridge to the conditional distribution \(\mu_{1 \mid 0}(\dd{\xi_{1}}, \xi_{0})\).

We define \textit{conditional velcoity} and \textit{denoiser} functions \(\zeta, \eta: [0, 1] \times H \times H \to H\) to be the following conditional expectations:

\begin{align}
  \zeta(t, x_{0}, x) &\coloneqq \mathop{\mathbb{E}}\qty[\dot{x}_{t} \mid \xi_{0} = x_{0}, x_{t} = x],\label{eqn:condzetadef} \\
  \eta(t, x_{0}, x) &\coloneqq \mathop{\mathbb{E}}\qty[ z \mid \xi_{0} = x_{0}, x_{t} = x]. \label{eqn:condetadef}
\end{align}

The conditional bridge is a stochastic process \(X_{t}\) governed by the following equation, which we call the CB-SDE:
\begin{equation}
  \dd{X_{t}} \coloneqq \qty( \zeta(t, \xi_{0}, X_{t}) - \frac{\varepsilon}{\gamma(t)} \eta(t, \xi_{0}, X_{t})) \dd{t} + \sqrt{2\varepsilon} \dd{W_{t}}, \quad X_{0} = \xi_{0}. \label{eqn:cbsde}
\end{equation}
We use the following to denote the drift coefficient of the CB-SDE:
\[
  f(t, x_{0}, x) \coloneqq \zeta(t, x_{0}, x) - \frac{\varepsilon}{\gamma(t)} \eta(t, x_{0}, x).
\]

For \(\mu_{0}\)-almost every \(\xi_{0}\), we denote by \(\mu_{t \mid 0}( \dd{x}, \xi_{0})\) the distribution of the interpolant \(x_{t}\), conditional on \(\xi_{0}\). Furthermore, assuming the CB-SDE (\ref{eqn:cbsde}) has a unique weak solution on a subinterval \([0, \overline{t}] \subseteq [0, 1]\), we let \(\rho_{t \mid 0}(\dd{x}, \xi_{0})\) be the law of \(X_{t}\) at time \(t \in [0, \overline{t}]\), conditional on \(X_{0} = \xi_{0}\).

We follow an analogous logic to the proof of \Cref{lem:fpmarg} to show that \(\rho_{t \mid 0}(\dd{x}, \xi_{0})\) and \(\mu_{t \mid 0}(\dd{x}, \xi_{0})\) are both solutions to a common Fokker-Plank equation with the following Kolmogorov operator indexed by \(\xi_{0}\):
\[
  \mathcal{L}_{\xi_{0}} u(t, x) \coloneqq \operatorname{Tr}\qty(\varepsilon C D^{2}_{x} u(t, x)) + D_{t}u(t, x) + \ev{f(t, \xi_{0}, x), D_{x} u(t, x)}_{H}.
\] Hence, the CB-SDE (\ref{eqn:cbsde}) is a suitable stochastic process where, conditional on a starting point \(X_{0} = \xi_{0}\), we may bridge to the conditional distribution \(\mu_{1 \mid 0}(\dd{\xi_{1}}, \xi_{0})\). We state this result directly blow, and provide a full proof in \Cref{prf:thm:cbsde}.

\begin{restatable}{theorem}{restatethmcbsde}\label{thm:cbsde}
  Let \(\mu_{t\mid 0}(\dd{x}, \xi_{0})\) be the law of the stochastic interpolant \(x_{t}\) at time \(t\), conditional on \(\xi_{0}\).
  \begin{enumerate}
    \item \label{ass:uniquelaw2} Suppose that for \(\mu_{0}\)-almost every initial condition \(X_{0} = \xi_{0}\), the CB-SDE (\ref{eqn:mbsde}) has solutions which are unique in law on a non-empty time interval \([0, \overline{t}] \subseteq [0, 1]\). We denote the law of \(X_{t}\) conditional on \(X_{0} = \xi_{0}\) by \(\rho_{t \mid 0}(\dd{x}, \xi_{0})\).
    \item \label{ass:densespace2} Suppose that for \(\mu_{0}\)-almost every \(\xi_{0}\), the set \(\mathcal{L}_{\xi_{0}}E\) is dense in \(L^{1}([0, \overline{t} ]\times H, \nu_{\xi_{0}})\), where \(\nu_{\xi_{0}}\) is the measure on \([0, \overline{t}] \times H\) determined uniquely by
      \[
        \nu_{\xi_{0}}(\dd{(t, x)}) = \nu_{\xi_{0}, t}(\dd{x}, \xi_{0}) \dd{t},
      \]
      and \(\nu_{\xi_{0}, t}(\dd{x}, \xi_{0}) \coloneqq \frac{1}{2} \rho_{t \mid 0}(\dd{x}, \xi_{0}) + \frac{1}{2} \mu_{t\mid 0}(\dd{x}, \xi_{0})\) for each \(t \in [0, \overline{t}]\).

  \end{enumerate}

  Then, for \(\dd{t}\)-almost every \(t \in [0, \overline{t}]\), we have
  \[
    \rho_{t \mid 0}(\dd{x}, \xi_{0}) = \mu_{t \mid 0}(\dd{x}, \xi_{0}).
  \]
\end{restatable}

Formally, the drift coefficient \(f(t, \xi_{0}, X_{t})\) is a \textit{random function} coupled to the specific initial condition \(X_{0} = \xi_{0}\). The uniqueness assumption (\ref{ass:uniquelaw2}) in \Cref{thm:cbsde} is hence identical to (\ref{ass:uniquelaw}) for the marginal bridge (Theorem \ref{thm:mbsde}) but restated to emphasise its dependence on this initial condition. In contrast, the dense range condition (\ref{ass:densespace2}) is necessarily stronger than its marginal counterpart (\ref{ass:densespace}) to ensure uniqueness for every conditional path.

The CB-SDE differs from the MB-SDE only in the inclusion of \(\xi_{0}\) as an additional conditioning variable when defining the conditional velocity and denoiser functions (Equations \ref{eqn:condzetadef} and \ref{eqn:condetadef}), which guarantee a bridge for each conditional path. To the best of our knowledge, this is the first statement of stochastic interpolants explicitly considers conditional paths between the source and target distributions. While \citet{albergo2023stochastic} consider SIs in which the source and target distributions are coupled, they do so to show that such a coupling provides simpler sampling paths, but without explicitly conditioning on the initial condition, their framework still only provides a marginal bridge. To illustrate this, we note that the CB-SDE and MB-SDE are equivalent when the following mean-independence conditions hold:
\begin{align*}
  \mathop{\mathbb{E}}\qty[\dot{x}_{t} \mid x_{t} = x] &= \mathop{\mathbb{E}}\qty[\dot{x}_{t} \mid \xi_{0} = x_{0}, x_{t} = x], \\
  \mathop{\mathbb{E}}\qty[z \mid x_{t} = x] &= \mathop{\mathbb{E}}\qty[z \mid \xi_{0} = x_{0}, x_{t} = x],
\end{align*}
that is, conditioning on \(\xi_{0}\) provides no further information than already provided by \(x_{t}\). This is a very strong statistical requirement which we do not assume. For example, these conditions are true when \(\xi_{0}\) is deterministic. %Nevertheless, when these conditions \textit{approximately} hold, omitting the conditioning on \(\xi_{0}\) may still give  % TODO: signpost our results that it is still promising

Since our primary focus is the application of SIs to forward and inverse problems, we henceforth center our analysis on the conditional bridge, with analogous results for the marginal bridge provided in the appendix. This approach is justified since the conditional bridge is a stronger construction: a bridge between marginal distributions can be readily recovered from the conditional bridge by marginalising over the source distribution. % TODO: provide marginal bridges in the appendix

We have established that conditional sample paths between source and target distributions can be obtained by solving the CB-SDE (\ref{eqn:cbsde}). To justify approximating (\ref{eqn:cbsde}) for conditional sampling, the next section ensures that a solution exists and is unique. This rules out spurious sample paths which result in a distribution other than \(\mu_{1\mid 0}(\dd{x}, \xi_{0})\).

% TODO: make sure at least this or previous section has note about epsilon=0

\section{Existence and Uniqueness of Strong Solutions}
While \Cref{thm:cbsde} only requires the existence and uniqueness of solutions to the CB-SDE in the weak sense, we focus on strong solutions to facilitate later analysis in \Cref{TODOsec:wasserstein} on the Wasserstein distance between generated samples and the true target distribution. This will allow us to use the same Wiener process to provide a coupling between the true CB-SDE with an SDE based on a drift learned by a neural network.

Our approach is to first show a result on the Lipschitz continuity of the drift coefficient \(f(t, x_{0}, x)\) as a function of \(x\) and use this result to show existence and uniqueness of strong solutions. We provide two different settings under which such a Lipschitz condition can be obtained.

In both settings, we assume the source and target data, \(\xi_{0}\) and \(\xi_{1}\), are supported on the Cameron-Martin space \(H_{C}\) of the covariance operator \(C\). This is a strong regularity condition which ensures the noise is inherently rougher than the data, allowing for the derivation of the well-defined posterior measures used for conditioning on \(x_{t}\) and \(\xi_{0}\).

The first setting directly addresses the case of Bayesian forward and inverse problems, in which we assume that the true data distribution \(\mu\) is supported on the Cameron-Martin space \(H_{C}\) and has a density with respect to a reference Gaussian measure. We state these conditions in the following hypothesis.

\begin{hypothesis}\label{hyp:bayes}
  Let \(H_{C} \coloneqq C^{\frac{1}{2}}H\) be the Cameron-Martin space of \(C\). We suppose the following conditions hold.
  \begin{enumerate}[label=\roman*]
    \item \label{hyp1.1} The law \(\mu\) of data \(\xi\) is supported on the product space \(H_{C}^{2} \coloneqq H_{C} \times H_{C} \) and has zero mean.
    \item \label{hyp1.2} \(\mu\) has a density \(p : H^{2}_{C} \to \mathbb{R}_{\geq 0}\) with respect to a \textit{prior} Gaussian measure \(\mathbb{P} \coloneqq N(0, Q)\) on \(H^{2}_{C}\), where \(Q\) is a positive-definite trace-class covariance operator on \(H_{C}^{2}\).
    \item \label{hyp1.3} The negative log-density \(\Phi \coloneqq - \log p\) is twice differentiable and strongly convex, that is, there exists a scalar \(k > 0\) where, for every \(\lambda \in [0, 1]\) and every \(u, v \in H^{2}_{C}\), we have
      \[
        \Phi(\lambda u + (1 - \lambda)v) \leq \lambda \Phi(u) + (1 - \lambda) \Phi(v) - \frac{k}{2}\lambda(1-\lambda) \norm{u - v}^{2}_{H^{2}_{C}}.
      \]
  \end{enumerate}
\end{hypothesis}

Using \Cref{hyp:bayes}, we establish the following result on the Lipschitz-continuity of the conditional expectation \(\mathop{\mathbb{E}}\qty[ \xi_{1} \mid \xi_{0}, x_{t}]\).

\begin{restatable}{proposition}{restatelembayes}\label{lem:bayes}
  Suppose \Cref{hyp:bayes} holds. Then the map \(x \mapsto f(t, x_{0}, x)\) is Lipschitz continuous with respect to the \(H_{C}\)-norm. Specifically, for each \(t \in (0, 1)\), \(x_{0} \in H_{C}\) and \(x \in H\), the following inequality holds:
  \[
    \norm{f(t, x_{0}, x) - f(t, x_{0}, y)}_{H_{C}} \leq L(t) \norm{x - y}_{H_{C}},
  \]
  where the Lipschitz constant \(L(t)\) is:
  \[
    L(t) =  \mathop{\underset{}{\max}}\qty{\abs{\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)}}, \abs{\dot{\beta}(t) - \beta\qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)})} \frac{\beta(t)}{\beta^{2}(t) + k \gamma^{2}(t)}}.
  \]
\end{restatable}
\begin{proof}[Proof (sketch)]
  The full proof is presented in \Cref{prf:lem:bayes} in Appendix \ref{app:A}.
  First, we re-express the drift to isolate its dependence on \(x\) into two terms: a linear term and the posterior conditional mean \(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0} = x_{0}, x_{t}= x]\). The problem thus reduces to proving that this conditional expectation is a Lipschitz-continuous map in \(x\).

  Our proof strategy uses a Galerkin-type argument in which we use a sequence of finite-dimensional approximations, combined with Brascamp-Lieb inequality \citep{brascamp1976extensions}. We introduce a sequence of approximating posterior measures defined on finite-dimensional subspaces \(H_{N} \subset H_{C}\). For each \(N\), we study the Frechet derivative of the approximate posterior mean on this subspace, with respect to \(x\), which is precisely the corresponding posterior covariance operator \(C_{N}\).

  The core of our contribution is the application of the Brascamp-Lieb inequality to this setting. This inequality provides an upper bound on the operator-norm of \(C_{N}\), in terms of the expectation of the inverse Hessian of the posterior log-density. By leveraging the strong convexity of the prior potential \(\Phi\), we establish a uniform lower bound on this Hessian in the Loewner order. This, in turn, yields a crucial upper bound on the operator norm of \(C_{N}\), independent of the dimension \(N\).

  This uniform bound on the norm of the derivative translates directly into a Lipschitz inequality with Lipschitz constant independent of \(N\). As we let \(N \to \infty\), we show that the approximate posterior means converge to the true posterior mean, which therefore inherits this uniform Lipschitz property.
\end{proof}

Our second setting replaces the density assumption on \(\mu\) with an assumption that is support is bounded. This approach is particularly useful when the data \(\xi = (\xi_{0}, \xi_{1})\) are subject to geometric constraints. For instance, if the data lie on a manifold, it may be natural to assume that their support is bounded.

% TODO: justify each of the assumptions

% TODO: add analogous result for marginal

% TODO: explain how you obtained the conditional distribution formula

% TODO: directly

% TODO: pf-ode must be nuanced here: conditional on X0, pf-ode will give a deterministic output

% TODO: emphasise sometimes that we write mu(dx) to emphasise the variable it describes

\begin{restatable}{proposition}{restatelemmanifold}\label{lem:manifold}
  Suppose the law \(\mu_{1}\) of the target data \(\xi_{1}\) is supported on a bounded subset of \(H_{C}\), that is, there exists a scalar \(R < \infty\) where \(\norm{\xi_{1}}_{H_{C}} < R\), \(\mu_{1}\)-almost surely. Then the map \(x \mapsto f(t, x_{0}, x)\) is Lipschitz continuous with respect to the \(H_{C}\)-norm. Specifically, for each \(t \in (0, 1)\) and \(x_{0}, x \in H\), the following inequality holds:
  \[
    \norm{f(t, x_{0}, x) - f(t, x_{0}, y)}_{H_{C}} \leq L(t) \norm{x - y}_{H_{C}},
  \]
  where the Lipschitz constant \(L(t)\) is:
  \[
    L(t) =  \mathop{\underset{}{\max}}\qty{\abs{\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)}}, \abs{\dot{\beta}(t) - \beta\qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)})} \frac{R^{2} \beta(t)}{\gamma^{2}(t)}}.
  \]
\end{restatable}
\begin{proof}
  The full proof is presented in \Cref{prf:lem:manifold} in Appendix \ref{app:A}. The overarching argument follows that of Lemma \ref{lem:bayes}, except that the argument is substantially simplified by the assumption that \(\xi_{1}\) has bounded support in \(H_{C}\), which allows a construction directly in infinite dimensions.
\end{proof}

Both cases involve the essential assumption that the target data \(\xi_{1}\) is supported on the Cameron-Martin space \(H_{C}\). This ensures, via the Cameron-Martin theorem, that the law of the interpolant \(x_{t}\) has a well-defined Radon-Nikodym derivative with respect to a reference measure, which acts as the likelihood function and in turn facilitates an expression for the density of the posterior law of \(\xi_{1}\) when conditioning on \(\xi_{0}=x_{0}\) and \(x_{t}=x\).

Intuitively, the restriction of \(\xi_{1}\) to \(H_{C}\) a smoothness assumption that confines realisations of \(\xi_{1}\) to a class of functions that are fundamentally less rough than typical realisations of the noise \(\gamma^{2}(t) z\). This assumption ensures that the laws Gaussian measures corresponding to translations of scaled-noise \(\gamma^{2}(t)\) by different candidates \(\xi_{1}', \xi_{1}''\) are always equivalent, allowing for an expression of the posterior measure as a well-defined density with respect to some reference measure. For instance, fix an initial state \(\xi_{0}\) and two candidates \(\xi_{1}', \xi_{1}''\). The likelihood of observing the interpolant \(x_{t}\) is given by the shifted Gaussian measures:
\[\operatorname{N}(\alpha(t) \xi_{0} + \beta(t) \xi_{1}, \gamma^{2}(t) C), \quad \xi_{1} = \xi_{1}',\xi_{1}''.\]
The Feldman-Hajek dichotomy states that two Gaussian measures are equivalent if and only if the difference in their means, \(\xi_{1}' - \xi_{1}''\), is in \(H_{C}\); otherwise they are mutually singular. Hence, if \(\xi_{1} \in H_{C}\) but \(\xi_{1}' \notin H_{C}\) with positive probability, then the supports of these two measures are disjoint, preventing the construction of a meaningful posterior measure as a density with respect to any reference measure.

% TODO: signpost choice of C

Building on  the  Lipshitz-continuity properties for the drift coefficient, established in Propositions \ref{lem:bayes} or \ref{lem:manifold}, we can now establish the existence of solutions to the CB-SDE (\ref{eqn:cbsde}). We first treat the issue of existence.

\begin{restatable}{theorem}{restatethmexist} \label{thm:exist}
  Suppose that there exists some \(\overline{t} \in (0, 1]\) such that for each \(t \in (0, \overline{t})\) and \(\mu_{0}\)-almost every \(x_{0}\), the mapping \(x \mapsto f(t, x_{0}, x)\) is Lipschitz continuous in \(H_{C}\) norm, satisfying
  \[
    \norm{f(t, x_{0}, x) - f(t, x_{0}, y)}_{H_{C}} \leq L(t) \norm{x - y}_{H_{C}}, \text{ for all } x, y \in H.
  \]
  for some function \(L(t)\). If \(L(t)\) is continuous on \((0, \overline{t})\) and integrable on \([0, \overline{t}]\), then there exists a strong solution to the CB-SDE (\ref{eqn:cbsde}) on the time interval \([0, \overline{t}]\).
\end{restatable}
\begin{proof}
  The full proof is presented in \Cref{prf:thm:exist} in Appendix \ref{app:A}. We allow for the possibility that \(\lim_{t \to 0} L(t) = +\infty\) or \(\lim_{t \to \overline{t}} L(t) = +\infty \) by only requiring an integrability condition for \(L(t)\) on \([0, \overline{t}]\) which allows us to consider a deterministic time change \(\hat{X}_{t} = X_{\theta(t)}\) (see Lemma \ref{lem:tchange} TODO) which leads to a finite Lipschitz constant on the new time domain.

  We prove existence for the well-behaved time-changed stochatic process using a piecewise construction: we partition of the new time domain into a finite sequence of small intervals in such a way that the Banach fixed-point theorem  yields the existencde of solutions on each subinterval. We then stitch these together to form a single, continuous strong solution for the time-changed process \(\hat{X}_{t}\). The adaptedness of this solution is preserved throughout the iterative construction. Finally, we recover the strong solution \(X_{t}\) to the original CB-SDE by reversing the time-change.
\end{proof}

Note that Banach's fixed point theorem does not guarantee uniqueness: the arguments we use in the proof only ensure uniqueness among solutions \(X_{t}\) where \(X_{t} - \xi_{0} - \sqrt{2\varepsilon} W_{t} \in H_{C}\). \textit{A priori}, we cannot  rule out  other solutions to the CB-SDE (\ref{eqn:cbsde}) that do not satisfy this condition.

To help facilitate our proof to the uniqueness of strong solutions to the CB-SDE (\ref{eqn:cbsde}), we use an additional decoupling assumption which ensures independence of components of \(\xi_{1}\) along the eigenvectors of the covariance operator \(C\).

\begin{restatable}{theorem}{restatethmuniq}\label{thm:uniq}
  Let \(\qty{e_{n}}_{n=1}^{\infty}\) be an orthonormal basis of eigenvectors for the covariance operator \(C\), and let \(H_{N}\) be the subspace of \(H_{C}\) spanned by \(\qty{e_{1}, \ldots, e_{N}}\). We denote by \(P_{N}\) the orthogonal projection operator from \(H\) into \(H_{N}\).

  Suppose that the distribution \(\mu_{1}\) of target data \(\xi_{1}\) is such that the projections \(\ev{\xi_{1}, e_{n}}\) are mutually independent random variables for different indices \(n\). Then, under the same Lipschitz continuity conditions as in \Cref{thm:exist}, the solution to the CB-SDE (\ref{eqn:cbsde}) is unique.
\end{restatable}
\begin{proof}
  The full proof is presented in \Cref{prf:thm:uniq} in Appendix \ref{app:A}. The decoupling assumption on the components of \(\xi_{1}\) allows us to employ a projection argument coupled with Gronwall's inequality to show that the norm of the difference between any two strong solutions driven by the same Wiener process is zero.
\end{proof}

% TODO: signpost that in section... come up with an example for which L(t) is integrable

\section{Parameterisation and Training Objective}
We now detail our choice of parameterisation in learning an approximation to the drift \(f(t,x_{0}, x)\)  of the CB-SDE (\ref{eqn:cbsde}). We propose decomposing the drift into two distinct components: a \textit{path velocity} \(\varphi\) and \textit{denoiser} \(\eta\):
\begin{equation}
  f(t, x_{0}, x) = \varphi(t, x_{0}, x) + \qty(\dot{\gamma}(t) - \frac{\varepsilon}{\gamma(t)}) \eta(t,  x_{0}, x), \label{eqn:driftgoodparam}
\end{equation}
where
\begin{align*}
  \varphi(t, x_{0}, x) &\coloneqq \mathop{\mathbb{E}}\qty[\dot{\alpha}(t) \xi_{0} + \dot{\beta}(t)\xi_{1} \,\big|\, \xi_{0} = x_{0}, x_{t} = x], \\
  \eta(t, x_{0}, x) &\coloneqq \mathop{\mathbb{E}}\qty[z \mid \xi_{0} = x_{0}, x_{t} = x].
\end{align*}
Hence, we decompose training into two learning objectives: one for \(\varphi\) and another for \(\eta\). This decomposition is natural, as the stochastic interpolant \(x_{t}\) comprises a  signal component \(\alpha(t) \xi_{0} + \beta(t) \xi_{1}\) and a noise component \(\gamma(t) z\). The path velocity \(\varphi\) captures the deterministic path conditioned on the datapoints \(\xi_{0}, \xi_{1}\), while the denoiser \(\eta\) controls the injection of stochasticity by the Wiener process. % todo: signpost

An alternative approach involves learning the drift via a single objective: estimating the conditional expectation \(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}, x_{t}]\) only. This is justified by the following decomposition of the drift:
\begin{align}
  f(t, x_{0}, x) &=  \qty(\dot{\alpha}(t) - \alpha(t)\qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)})) x_{0} \notag\\
  &\mathrel{\phantom{=}}\, + \qty(\dot{\beta}(t) - \beta(t)\qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)})) \mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0} = x_{0}, x_{t} = x]\notag \\
  &\mathrel{\phantom{=}}\, + \qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)}) x_{t}. \label{eqn:driftbadparam}
\end{align}
Although this is mathematically valid and useful for proving the theoretical bounds in \Cref{lem:bayes,lem:manifold}, we find that this parameterisation exhibits far weaker empirical performance compared to the decomposition into path velocity and denoiser. We attribute this underperformance to two primary factors:
\begin{enumerate}
  \item Inductive bias: our two-component approach provides a more effective inductive bias. The task of directly predicting the target \(\xi_{1}\) given \((\xi_{0}, x_{t})\) is more complex than the two sub-problems of learning the path velocity and the denoiser. By simplifying the learning task, our decomposition helps the model find a better approximation of the overall drift.
  \item Numerical stability: the alternative parameterisation suffers from severe instabilities due to the singularities at times \(t = 0 ,1\) that amplify approximation errors. This makes sampling unreliable at these critical times.
\end{enumerate}

The crucial difference lies in the severity of the endpoint singularities. In our proposed parameterisation (Equation \ref{eqn:driftgoodparam}),  the coefficient \(\dot{\gamma}(t) - \frac{\varepsilon}{\gamma(t)}\) on the denoiser \(\eta(t, x_{0}, x)\) is also singular at \(t = 0, 1\). However this coefficient is integrable on \([0, 1]\) as long as \(\frac{1}{\gamma(t)}\) is integrable. This property allows us to mitigate sampling instabilities by introducing a change of time (see Lemma \ref{sec:timechange} TODO).

In contrast, the coefficient in the alternative parameterisation (Equation \ref{eqn:driftbadparam}) includes an additional \(\frac{1}{\gamma(t)}\) factor which results in a stronger singularity that is non-integrable on \([0, 1]\) for any choice of \(\gamma(t)\). This argument is formalised in \Cref{sec:nonintegrable} in Appendix \cref{app:A}, and means that the alternative parameterisation is fundamentally less stable and cannot be resolved by a similar time-change technique.

Having established our choice in parameterising the drift \(f(t, x_{0}, x)\) as a decomposition into a path velocity term \(\varphi(t, x_{0}, x)\) and denoiser term \(\eta(t, x_{0}, x)\), we introduce our loss functions. We will consider losses with respect to both the \(H\)-norm and the \(H_{C}\)-norm, and hence introduce the variable \(U\) as a Hilbert space representing either \(H\) or \(U\). For approximations \(\widetilde{\varphi}\) and \(\widetilde{\eta}\), we define the \textit{true path matching} (TPM) and \textit{true denoiser matching} (TDM) objectives below:
\begin{align}
  \mathrm{TPM}_{t}(\widetilde{\varphi}) &\coloneqq \mathop{\mathbb{E}}\qty[\norm{ \widetilde{\varphi}(t, \xi_{0}, x_{t}) - \varphi(t, \xi_{0}, x_{t})}^{2}_{U}]\label{eqn:tpm} \\
  \mathrm{TDM}_{t}(\widetilde{\eta}) &\coloneqq \mathop{\mathbb{E}}\qty[\norm{ \widetilde{\eta}(t, \xi_{0}, x_{t}) - \eta(t, \xi_{0}, x_{t})}^{2}_{U}] \label{eqn:tdm}
\end{align}
In practical terms, we do not have access to the ground-truth conditional expectations needed to calculate the TPM and TDM losses. Hence, we introduce two auxiliary losses, the \textit{practical path matching} (PPM) and \textit{practical denoiser matching} (PDM) objectives below:
\begin{align}
  \mathrm{PPM}_{t}(\widetilde{\varphi}) &\coloneqq \mathop{\mathbb{E}}\qty[\norm{ \widetilde{\varphi}(t, \xi_{0}, x_{t}) - (\dot{\alpha}(t) \xi_{0} + \dot{\beta}(t) \xi_{1})}^{2}_{U}] \label{eqn:ppm}\\
  \mathrm{PDM}_{t}(\widetilde{\eta}) &\coloneqq \mathop{\mathbb{E}}\qty[\norm{ \widetilde{\eta}(t, \xi_{0}, x_{t}) - z}^{2}_{U}] \label{eqn:pdm}
\end{align}

These losses are analogous to technique employed when training stochastic interpolants in finite dimensions (see \citealp[][Theorems 2.7--2.8]{albergo2023stochasticinterpolantsunifyingframework}), which makes the loss functions tractable by replacing the target conditional expectations with a sample of the underlying random variable to form a practical loss objective. However, in infinitte dimensions, the true matching objectives could finite while the practical objectives are not: special care must be taken to ensure that both sets of losses are finite. This is established in the next result.

\begin{restatable}{proposition}{restateprploss}\label{prp:loss}
  Let \(U\) be the Hilbert space \(H\) in the definitions of the true objectives (Equations \ref{eqn:tpm} and \ref{eqn:tdm}) and practical objectives (Equations \ref{eqn:ppm} and \ref{eqn:pdm}). Given candidate approximations \(\widetilde{\varphi}\) and \(\widetilde{\eta}\) for which the TPM and TDM objectives are finite, the practical objectives \(\mathrm{PPM}_{t}(\widetilde{\varphi})\) and \(\mathrm{PDM}_{t}(\widetilde{\varphi})\) differ from \(\mathrm{TPM}_{t}(\widetilde{\varphi})\) and \(\mathrm{TDM}_{t}(\widetilde{\varphi})\) only by a finite constant for any \(t \in (0, 1)\).

  Furthermore, if \(U\) is instead the subspace \(H_{C}\), the same result is true if the target data \(\xi_{1}\) is supported on \(H_{C}\) and has finite second moment, that is, \( \mathop{\mathbb{E}}\qty[\norm{\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}, x_{t}] - \xi_{1}}_{H_{C}}^{2}] < \infty\).
\end{restatable}
\begin{proof}
  The full proof is given in \Cref{prf:prp:loss} in Appendix \ref{app:A}.
\end{proof}
% + time change

% + bound to distance target measure

Note that under both settings of \Cref{lem:bayes} or \Cref{lem:manifold}, the target data \(\xi_{1}\) is supported on \(H_{C}\) and has finite second moment. Hence, \Cref{prp:loss} shows that both the \(H\)-norm and \(H_{C}\)-norm are valid choices for our training objectives.

Having established conditions under which our training objectives are well-defined in infinite dimensions, we now quantify the quality of samples generated by simulating the CB-SDE (\ref{eqn:cbsde}) when the drift coefficient \(f\) is replaced by a learned drift \(\widetilde{f}\), compared with the true conditional law \(\mu_{t \mid 0}(\dd{x_{t}}, \xi_{0})\).

\begin{theorem}
  TODO: result regarding Wasserstein-2 distance. This will be stated in \(U\)-norm \(U = H\) or \(H_{C}\) This requires a result on Lipschitz continuity in \(U\)-norm. Note that all the proofs we had so far are in \(H_{C}\)-norm.
\end{theorem}

We acknowledge a subtle but important distinction between our theoretical analysis and practical implementation. The Lipschitz continuity results established in \Cref{lem:bayes,lem:manifold} are derived with respect to the \(H_{C}\)-norm. An ideal training procedure would therefore employ loss functions measured in the \(H_{C}\)-norm, that is, apply \Cref{eqn:ppm,eqn:pdm} with \(U = H_{C}\), to align directly with these guarantees.

However, implementing such a loss is computationally demanding, as it requires access to the inverse covariance operator, \(C^{-1}\). For tractability, we instead adopt the standard \(H\)-norm for our training objectives, which translates to standard mean-squared-error loss in implementation. A crucial consequence of this choice is that minimizing the loss in \(H\)-norm does not guarantee control on the corresponding loss in \(H_{C}\)-norm: one could have an arbitrarily small but positive \(H\)-norm loss that has an unbounded loss in \(H_{C}\) norm, since  high-frequency components of the learned functions, by which we mean components in directions of eigenvectors of \(C^{-1}\) for which the corresponding eigenvalue is large, are strongly penalised in \(H_{C}\)-norm  but not in \(H\)-norm. This observation suggests that a promising direction for future work is the inclusion of an explicit regularization term that penalizes high-frequency outputs when training with an \(H\)-norm loss.

TODO: note that the Lipschitz constant is finite on \([0,1)\) when \(\gamma(t) = \sqrt{bt(1-t)}\).

\section{Change of time}
TODO. Integrability of \(\frac{1}{\gamma(t)}\) implies \(\exists \) of time change that makes coefficient on \(\eta(t, \xi_{0}, x_{t})\) not blow up on \([0, 1]\) and hence

\section{Algorithms}
TODO. Describe the use of predictor-corrector.

Also consider the ODE and argue why the SDE is better (\(\varepsilon > 0\) helps to control the injection of noise by the Wiener process and "correct" learning errors; can refer to the EDM paper for diffusion)

\section{Backwards SDE}
TODO: note that CB-SDE involves training two separate models for the forwrad/inverse tasks. In constrast, the MB-SDE allows for the same model to be used for both tasks.

Empirically, we find that there is only a marginal loss in performance, even though the MB-SDE does not guarantee a bridge to the \textit{conditional} distribution. Suggests most information captured by \(x_{t}\).