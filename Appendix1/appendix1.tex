%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\chapter{Mathematical Proofs} \label{app:A}

\section{Proof of \Cref{lem:fpmarg}}\label{prf:lem:fpmarg}
\restatelemfpmarg*

\begin{proof}
  It is sufficient to restrict our attention to any real-valued test function of the form \(u(t, x) = \operatorname{Re}\qty[\phi(t) e^{i \ev{x, h(t)}_{H}}]\) or \(\operatorname{Im}\qty[\phi(t) e^{i \ev{x, h(t)}_{H}}]\), where \(\phi\) and \(h\) satisfy the properties given in \Cref{eqn:testfns}.

  Fix \(t \in [0,1]\) and consider the characteristic function of the real-valued random variable \(u(t, x_{t})\). For any \(k \in \mathbb{R}\), we define
  \begin{align}
    \chi(t, k) &\coloneqq \mathop{\mathbb{E}}\qty[e^{i k u(t, x_{t})}]
  \end{align}
  Taking derivatives with respect to \(t\) and \(k\) and evaluating at \(k=0\) allows us to compute the time derivative of the expected value of \(u(t, x_{t})\):
  \begin{equation}
    %   \pdv{t}\chi(t, k) &= i k \mathop{\mathbb{E}}\qty[e^{i k u(t, x_{t})} \qty(D_{t} u(t, x_{t}) + \ev{ \dot{x}_{t}, D_{x}u(t, x_{t})}_{H})] \\
    \eval{\frac{1}{i} \pdv[2]{}{t}{k} \chi(t, k)}_{k=0} = \dv{t} \mathop{\mathbb{E}}\qty[u(t, x_{t})] = \mathop{\mathbb{E}}\qty[D_{t} u(t, x_{t}) + \ev{ \dot{x}_{t}, D_{x}u(t, x_{t})}_{H}].
  \end{equation}

  Since the inner product \(\ev{ \dot{x}_{t}, D_{x} u(t, x_{t})}_{H}\) is linear in its first argument, we may apply the law of iterated expectations and replace \(\dot{x}_{t}\) with \(\zeta(t, x_{t}) = \mathop{\mathbb{E}}\qty[ \dot{x}_{t} \mid x_{t}]\) as defined in \Cref{eqn:zetadef}:
  \[
    \dv{t} \mathop{\mathbb{E}}\qty[u(t, x_{t})] = \mathop{\mathbb{E}}\qty[D_{t} u(t, x_{t}) + \ev{  \zeta(t, x_{t}), D_{x} u(t, x_{t})}_{H}]
  \]

  Adding and subtracting \(\frac{\varepsilon}{\gamma(t)} \eta(t, x_{t})\), where \(\eta(t, x_{t}) = \mathop{\mathbb{E}}\qty[ z \mid x_{t}]\) as defined in \Cref{eqn:etadef}, we have
  \begin{align}
    \dv{t} \mathop{\mathbb{E}}\qty[u(t, x_{t})] &= \mathop{\mathbb{E}}\qty[D_{t} u(t, x_{t}) + \ev{ \frac{\varepsilon}{\gamma(t)}\eta(t, x_{t}) + \zeta(t, x_{t}) - \frac{\varepsilon}{\gamma(t)} \eta(t, x_{t}), D_{x} u(t, x_{t})}_{H}] \notag \\
    &= \frac{\varepsilon}{\gamma(t)}\mathop{\mathbb{E}}\qty[\ev{z, D_{x}u(t, x_{t})}_{H}] + \mathop{\mathbb{E}}\qty[ D_{t}u(t, x_{t}) + \ev{f(t, x_{t}), D_{x} u(t, x_{t})}_{H}], \label{eqn:simpl}
  \end{align}
  where we simplified the first term using the law of iterated expectations to simplify the first term, and substituted the definition \(f(t, x) = \zeta(t, x) - \frac{\varepsilon}{\gamma(t)} \eta(t, x)\) given in \Cref{eqn:deff} for the second term.

  For the following, we assume that \(u(t, x) = \operatorname{Re}[\phi(t) e^{i \ev{x, h(t)}_{H}}]\), but an identical line of reasoning applies if \(u(t, x) = \operatorname{Im}\qty[\phi(t) e^{i \ev{x, h(t)}_{H}}]\).

  Let us focus on the first term in \Cref{eqn:simpl}. We have:
  \begin{align}
    \frac{\varepsilon}{\gamma(t)} \mathop{\mathbb{E}}[\ev{z, D_{x} u(t, x_{t})}_{H}] &= \operatorname{Re}\qty[i \frac{\varepsilon}{\gamma(t)} \mathop{\mathbb{E}}\qty[\phi(t) e^{i \ev{x_{t}, h(t)}_{H}} \ev{z, h(t)}_{H}]] \notag \\
    &= \operatorname{Re}\qty[i \frac{\varepsilon}{\gamma^{2}(t)} \mathop{\mathbb{E}}\qty[\phi(t) e^{i \ev{\alpha(t) \xi_{0} + \beta(t) \xi_{1}, h(t)}_{H}}] \mathop{\mathbb{E}}\qty[e^{i \ev{\gamma(t) z, h(t)}_{H}} \ev{\gamma(t) z, h(t)}_{H}]], \label{eqn:nearlythere}
  \end{align}
  where the second line follows since \(z \perp (\xi_{0}, \xi_{1})\).

  Let \(\qty{\lambda_{n}, e_{n}}_{n=1}^{\infty}\) be an orthonormal system for \(C\) (i.e. \(C e_{n} = \lambda e_{n}\) for each \(n\)) and define the scalar-valued functions \(h_{n}(t) \coloneqq \ev{h(t), e_{n}}_{H}\). The projections \(z_{n} = \ev{z, e_{n}}\) for each \(n\) are mutually independent 1-dimensional Gaussians with zero mean and variances equal to \(\lambda_{n}\).  By Parseval's theorem, we have the identity \(\ev{\gamma(t) z, h(t)} = \sum_{n=1}^{\infty} \gamma(t) h_{n}(t) z_{n} \). We may therefore write
  \[
    \mathop{\mathbb{E}}\qty[\ev{\gamma(t) z, h(t)}_{H} e^{i \ev{\gamma(t)z, h(t)}_{H}}] = \sum_{n=1}^{\infty} \mathop{\mathbb{E}}\qty[\gamma(t) h_{n}(t) z_{n} e^{i \gamma(t) h_{n}(t) z_{n}}] \prod\limits_{m \neq n} \mathop{\mathbb{E}}\qty[e^{i \gamma(t) h_{m}(t) z_{m}}]
  \]
  Using the identity \(\mathop{\mathbb{E}}\qty[q e^{i q}] = i v \mathop{\mathbb{E}}\qty[e^{i q}]\) for a 1-dimensional Gaussian \(v \sim \operatorname{N}(0, q)\), we have
  \begin{align*}
    \mathop{\mathbb{E}}\qty[\ev{\gamma(t) z, h(t)}_{H} e^{i \ev{\gamma(t)z, h(t)}_{H}}] = \sum_{n=1}^{\infty} i \gamma^{2}(t)h_{n}^{2}(t)\lambda_{n} \mathop{\mathbb{E}}\qty[e^{i \ev{\gamma(t) z, h(t)}_{H}}]
  \end{align*}
  Substituting into \Cref{eqn:nearlythere}, we have
  \begin{align*}
    \frac{\varepsilon}{\gamma(t)} \mathop{\mathbb{E}}\qty[\ev{z, D_{x} u(t, x_{t})}_{H}] &= \mathop{\mathbb{E}}\qty[\sum_{n=1}^{\infty} - \varepsilon \lambda_{n} h_{n}^{2}(t) u(t, x_{t})] = \mathop{\mathbb{E}}\qty[\operatorname{Tr}\qty(\varepsilon C D^{2}_{x}u(t, x_{t}))].
  \end{align*}
  Finally, substituting this expression into \Cref{eqn:simpl} and re-writing expectations via integrals, we have
  \[
    \dv{t} \int_{H} u(t, x) \mu_{t}(\dd{x}) = \int_{H} \operatorname{Tr}\qty(\varepsilon C D^{2}_{x} u(t, x)) + D_{t}u(t, x) + \ev{f(t, x), D_{x}u(t, x)}_{H} \mu_{t}(\dd{x}).
  \]
  Since the choice of \(t\) was arbitrary, it follows that \(\mu_{t}\) satisfies the Fokker-Plank equation (\ref{eqn:fp}) for any \(t \in [0, t]\). This concludes the proof.
\end{proof}

\section{Proof of \Cref{thm:mbsde}}\label{prf:thm:mbsde}
\restatethmmbsde*
\begin{proof}
  In addition to \(\nu\), we define the measures \(\rho\) and \(\mu\) on the product space \([0, \overline{t}] \times H\) determined uniquely by \(\rho(\dd{(t, x)}) = \rho_{t}(\dd{x}) \dd{t}\) and \(\mu(\dd{(t, x)}) = \mu_{t}(\dd{x}) \dd{t}\). Hence, it follows by construction that \(\nu = \frac{1}{2} \rho + \frac{1}{2} \nu\) and both \(\rho\)and \(\mu\) are absolutely continuous with respect to \(\nu\). We define their densities \(p, q\) with respect to \(\nu\):
  \[
    p(t, x) \coloneqq \dv{\rho}{\nu}(t, x) \qand q(t, x) = \dv{\mu}{\nu}(t, x).
  \]
  From \Cref{lem:fpmarg} we know that both \(\rho_{t}\) and \(\mu_{t}\) solve the Fokker-Plank equation (\ref{eqn:fp}). Hence,
  \begin{equation}
    0 = \int_{[0, \overline{t}] \times H} \mathcal{L} u(t, x) (p(t, x)  -q(t, x)) \nu(\dd{(t, x)}) \label{eqn:fpuniq}
  \end{equation}
  for every test function \(u \in E\). Note that for \(\nu\)-almost every \((t, x)\), we have \(0 \leq p(t, x), q(t, x) \leq 2\), so their difference is bounded almost everywhere. Since \Cref{eqn:fpuniq} holds for every \(u \in E\) and by assumption, \(\mathcal{L} E\) is dense in \(L^{1}([0, \overline{t}] \times H, \nu)\), it follows that
  \[
    p(t, x) = q(t, x)
  \]
  for \(\nu\)-almost every \((t, x)\). Hence, the signed measure \(\rho - \mu = 0\) and \(\rho_{t} = \mu_{t}\) for \(\dd{t}\)-almost every \(t\). This  concludes the proof.
\end{proof}
% TODO: remove eqn numbers where they are redundant

% TODO: ensure all ev's have a _H or H_c, etc

\section{Proof of \Cref{lem:bayes}}\label{prf:lem:bayes}
\restatelembayes*

% TODO: must summarise steps in the proof
\begin{proof}
  The proof proceeds in steps TODO
  \paragraph{Step 0}
  First, we notice that the drift term can be re-written:
  \begin{align}
    f(t, x_{0}, x) &= \mathop{\mathbb{E}}\qty[\dot{\alpha}(t) \xi_{0} + \dot{\beta}(t) \xi_{1} + \qty(\dot{\gamma}(t) - \frac{\varepsilon}{\gamma(t)}) z \,\bigg|\, \xi_{0} = x_{0}, x_{t} = x] \notag\\
    &= \qty(\dot{\alpha}(t) - \alpha(t)\qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)})) x_{0} \notag\\
    &\mathrel{\phantom{=}}\, + \qty(\dot{\beta}(t) - \beta(t)\qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)})) \mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0} = x_{0}, x_{t} = x]\notag \\
    &\mathrel{\phantom{=}}\, + \qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)}) x_{t}.\label{eqn:driftreexpressed}
  \end{align}
  Hence, if we can show that the mapping \(x \mapsto \mathop{\mathbb{E}}\qty[\xi_{1} \mid\xi_{0} = x_{0}, x_{t} = x]\) is Lipschitz continuous in \(H_{C}\)-norm, this translates to Lipschtiz continuity in the overall mapping \(x \mapsto f(t, x_{0}, x)\).

  \paragraph{Step 1}
  Let \(\mu_{1 \mid 0, t}(\dd{\xi_{1}}, x_{0}, x)\) denote the posterior law of \(\xi_{1}\), conditional on \(\xi_{0} = x_{0}\) and \(x_{t} = x\). Furthermore, let \(\mathbb{P}_{1 \mid 0}(\dd{\xi_{1}}, x_{0})\) be the corresponding conditional prior, which is a well-defined Gaussian measure on \(H_{C}\)  (see, e.g., \citealp[][Chapter 3.10]{bogachev1998gaussian}). We use \(m_{1 \mid 0}(x_{0})\) and \(Q_{1 \mid 0}\) respectively to denote the mean and covariance operator of this Gaussian on \(H_{C}\). Note that the prior conditional mean \(m_{1 \mid 0}(x_{0})\) is a linear function of \(x_{0}\). Then for \(\mu_{0}\)-almost every \(x_{0} \in H_{C}\), the law \(\mu_{1 \mid 0, t}(\dd{\xi_{1}, x_{0}, x})\) is absolutely continuous with respect to the reference measure \(\mathbb{P}_{1 \mid 0}(\dd{\xi_{1}}, x_{0})\) with the following density:
  \begin{align*}
    \dv{\mu_{1 \mid 0, t}(\cdot, x_{0}, x)}{\mathbb{P}_{1 \mid 0}(\cdot, x_{0})}{}(\xi_{1}) &= \frac{1}{Z_{1 \mid 0,t}(x_{0}, x)}\exp(- V_{1 \mid 0, t}(\xi_{1}, x_{0}, x)),\\
    \text{ where } V_{1 \mid 0, t}(\xi_{1}, x_{0}, x) &\coloneqq \frac{1}{2\gamma^{2}(t)} \norm{\alpha(t) x_{0}+ \beta(t) \xi_{1} - x}_{H_{C}}^{2} + \Phi(x_{0}, \xi_{1}),
  \end{align*}
  and \(Z_{1 \mid 0,t}(x_{0}, x) \coloneqq \int_{H_{C}} \exp(- V_{1 \mid 0, t}(\xi_{1}, x_{0}, x)) \mathbb{P}_{1 \mid 0} (\dd{\xi_{1}, x_{0}})\) is a normalising constant.
  \paragraph{Step 2} Let \(\qty{e_{n}}_{n=1}^{\infty}\) be an orthonormal basis for \(H_{C}\) and for each \(N \geq 1\), let \(H_{N}\) be the linear span of \(\qty{e_{1}, \ldots, e_{N}}\). We define \(\Pi_{N} : H_{C} \to H_{N}\) as the self-adjoint orthogonal projection operator onto the finite-dimensional subspace \(H_{N}\) of \(H_{C}\) and let \(\xi_{1,N} \coloneqq \Pi_{N} \xi_{1}\). Furthermore, we define a reference measure by projecting \(\mathbb{P}_{1 \mid 0}\) onto this subspace:
  \begin{align*}
    \mathbb{P}_{1 \mid 0, N}(\dd{\xi_{1, N}}, x_{0}) &\coloneqq \operatorname{N}(m_{1 \mid 0, N}(x_{0}),  Q_{N}), \\
    \text{ where } m_{1 \mid 0, N}(x_{0}) &\coloneqq \Pi_{N} m_{1 \mid 0}(x_{0}), \\
    \text{ and } Q_{N} &\coloneqq \Pi_{N} Q_{1 \mid 0} \Pi _{N}.
  \end{align*}
  Using this, we create a sequence of approximating posterior measures \(\mu_{1 \mid 0, t, N}\) by restricting the potential to \(H_{N}\): for each \(\xi_{1,N} \in H_{N}\).
  \begin{align*}
    \dv{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}{\mathbb{P}_{1 \mid 0, N}(\cdot, x_{0})}{}(\xi_{1,N}) &\coloneqq \frac{1}{Z_{1 \mid 0, t, N}(x_{0}, x)} \exp(-V_{1 \mid 0, t, N}(\xi_{1,N}, x_{0}, x)), \\
    \text{ where } V_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x) &\coloneqq  \frac{1}{2\gamma^{2}(t)} \norm{\alpha(t) \Pi_{N} x_{0} + \beta(t) \xi_{1, N} - x}^{2}_{H_{C}} + \Phi(x_{0}, \xi_{1, N}),
  \end{align*}
  where \(Z_{1 \mid 0, t, N}(x_{0}, x) \coloneqq \int_{H_{N}} \exp(-V_{1 \mid 0, t, N})(\xi_{1, N}, x_{0}, x) \mathbb{P}_{1 \mid 0, N}(\dd{\xi_{1, N}, x_{0}})\) is a normalising constant.

  Given these definitions, we study the following approximation of the posterior mean:
  \begin{equation}
    m_{1 \mid 0, t, N}(x_{0}, x) \coloneqq \mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[\xi_{1, N}] = \int_{H_{N}} \xi_{1, N}\mu_{1 \mid 0, t, N}(\dd{\xi_{1, N}}, x_{0}, x).\label{eqn:apm}
  \end{equation}
  We aim to find a Lipschitz constant for the map \(x \mapsto m_{1 \mid 0, t, N}(x_{0}, x)\) that is independent of \(N\) and \(x_{0}\). To do so, we consider the Frechet derivative of \(m_{1 \mid 0, t, N}(x_{0}, x)\) with respect to \(x\), applied in a direction \(h \in H_{C}\). This is a covariance (see Lemma \ref{lem:frechetf}):

  \begin{align}
    D_{x} m_{1 \mid 0, t, N}(x_{0}, x)[h] &=\frac{\beta(t)}{\gamma^{2}(t)} \mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x)) \ev{\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x), h}_{H_{C}}] \notag \\
    &=\frac{\beta(t)}{\gamma^{2}(t)} \mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x)) \ev{\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x), \Pi_{N} h}_{H_{N}}] \label{eqn:dxmt0},
  \end{align}
  where the second equality follows from the first since the components of \(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x)\) along the basis vectors \(\qty{e_{n}}_{n=N+1}^{\infty}\) are all zero.

  By the Riesz representation theorem, the \(N\)-dimensional subspace \(H_{N}\) is isomorphic with \(\mathbb{R}^{N}\), so all vectors on \(H_{N}\) can be identified with an \(N\)-dimensional column vector in \(\mathbb{R}^{N}\). We may therefore re-write the derivative using an \(N\)-dimensional covariance matrix \(C_{N}\) acting on the vector \(\Pi_{N} h\):
  \begin{align}
    D_{x} m_{1 \mid 0, t, N}(x_{0}, x)[h] &= \frac{\beta(t)}{\gamma^{2}(t)} C_{N} \Pi_{N} h, \label{eqn:dxmt}\\
    \text{ where } C_{N} &= \mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty((\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x))(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x))^{\tran}). \notag
  \end{align}
  For the rest of the proof, we identify \(C_{N}\) with a self-adjoint covariance operator on \(H_{N}\).

  \paragraph{Step 3}
  We now use the Brascamp-Lieb inequality \citep{brascamp1976extensions} to place a bound on the operator norm of \(C_{N}\). We proceed by expressing the approximate posterior measure \(\mathbb{\mu}_{1 \mid 0, t, N}(\dd{\xi_{1, N}}, x_{0}, x)\) via a density relative to the Lebesgue measure on \(H_{N}\) (identified with \(\mathbb{R}^{N}\)). The density of the reference measure \(\mathbb{P}_{1 \mid 0, N}(\dd{\xi_{1, N}}, x_{0})\) with respect to the Lebesgue measure, evaluated at \(\xi_{1, N} \in H_{N}\), is proportional to
  \[\exp(-\frac{1}{2} \ev{Q_{N}^{-1} (\xi_{1, N} - m_{1 \mid 0, N}(x_{0})), \xi_{1, N} - m_{1 \mid 0, N}(x_{0})}_{H_{N}}),
  \]
  where the inverse \(Q_{N}^{-1}\) is well-defined because \(Q_{N} : H_{N} \to H_{N}\) is positive-definite and bounded. Hence, % TODO: refer back to the theorem
  \begin{align*}
    &\mu_{1 \mid 0, t, N}(\dd{\xi_{1, N}}, x_{0}, x) \\
    &\quad\propto \exp(-V_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x) - \frac{1}{2} \ev{Q_{N}^{-1} (\xi_{1, N} - m_{1 \mid 0, N}(x_{0})), \xi_{1, N} - m_{1 \mid 0, N}(x_{0})}_{H_{N}}) \dd{\xi_{1, N}}.
  \end{align*}
  Let \(W_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x) \coloneqq V_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x) + \frac{1}{2} \ev{Q_{N}^{-1} (\xi_{1, N} - m_{1 \mid 0, N}(x_{0})), \xi_{1, N} - m_{1 \mid 0, N}(x_{0})}_{H_{N}}\)  be the total potential with respect to the Lebesgue measure on \(H_{N}\). Since this is twice-differentiable and strictly convex, the conditions for the Brascamp-Lieb inequality are satisfied (see \citep[][Theorem 4.1]{brascamp1976extensions}): for any continuously differentiable function \(f : H_{N} \to \mathbb{R}\), we have
  \begin{align*}
    &\mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[\qty(f(\xi_{1, N}) - \bar{f})^{2}]\\
    &\quad\leq \mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[\ev{ \qty(D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x))^{-1} D f(\xi_{1, N}), D f(\xi_{1, N}) }_{H_{N}}],
  \end{align*}
  where \(\overline{f}\) is the expectation of \(f(\xi_{1, N})\) under the measure \(\mu_{1 \mid 0, t, N}(\dd{\xi_{1, N}, x_{0}, x})\) and \(D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x)\) is the inverse Hessian of \(W_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x)\) with respect to \(\xi_{1, N}\) on \(H_{N}\). In the case where \(f(\xi_{1, N}) = \ev{\xi_{1, N}, u}_{H_{N}}\) for any \(u \in H_{N}\), we have \(Df(\xi_{1, N}) = u\), and
  \begin{align}
    &\mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[\qty(f(\xi_{1, N}) - \bar{f})^{2}] = \ev{C_{N}u, u} \notag\\
    &\qquad\qquad\leq  \mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[\ev{ \qty(D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x))^{-1} u, u }_{H_{N}}]. \label{eqn:brascamp}
  \end{align}
  \paragraph{Step 4}
  We now aim to place a Loewner order on the inverse Hessian \(\qty(D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x))^{-1}\) irrespective of \(\xi_{1, N}\), which will in turn allow us to form a Loewner order on \(C_{N}\).

  Taking the second-order Frechet derivatives of \(W_{1 \mid 0, t,N}(\xi_{1, N}, x_{0}, x)\) with respect to \(\xi_{1, N}\) in the directions \(u, v \in H_{N}\), we have
  \begin{align*}
    D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{N}, x_{0}, x)[u, v] = \ev{\qty(\frac{\beta^{2}(t)}{\gamma^{2}(t)} I_{N} + \Pi_{N}\grad_{\xi_{1}}^{2}\Phi(x_{0}, \xi_{1, N})\Pi_{N} + Q_{N}^{-1})u, v}_{H_{N}},
  \end{align*}
  where \(\grad_{\xi_{1}}^{2} \Phi(\xi_{0}, \xi_{1})\) is the partial Hessian of the potential \(\Phi\) with respect to the second coordinate. This allows us to identify the Hessian with a self-adjoint Hessian operator from \(H_{N}\) to \(H_{N}\):
  \begin{equation}
    D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{N}, x_{0}, x)[u, v] = \frac{\beta^{2}(t)}{\gamma^{2}(t)} I_{N} + \Pi_{N}\grad_{\xi_{1}}^{2}\Phi(x_{0}, \xi_{1, N})\Pi_{N} + Q_{N}^{-1}\label{eqn:hess}
  \end{equation}
  Since \(\Phi\) is \(k\)-strongly convex, it is also \(k\)-strongly convex in the second coordinate and hence the projection of its partial Hessian satisfies the following Loewner order:
  \[
    \Pi_{N} \grad^{2}_{\xi_{1}} \Phi(x_{0}, \xi_{1, N}) \succcurlyeq k I_{N},
  \]
  which allows us to place a Loewner order on \Cref{eqn:hess}:
  \[
    D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{N}, x_{0}, x)[u, v] \succcurlyeq \qty(\frac{\beta^{2}(t)}{\gamma^{2}(t)} + k) I_{N} + Q_{N}^{-1}
  \]
  Since the right-hand side of this quantity is positive-definite, this Loewner order is reversed when taking inverses:
  \[
    \qty(D^{2}_{\xi_{1, N}}W_{1 \mid 0, t, N}(\xi_{N}, x_{0}, x)[u, v])^{-1} \preccurlyeq \qty(\qty(\frac{\beta^{2}(t)}{\gamma^{2}(t)} + k) I_{N} + Q_{N}^{-1})^{-1}.
  \]
  This relationship holds uniformly for all \(\xi_{1, N} \in H_{N}\). Substituting into \Cref{eqn:brascamp}, we have
  \begin{align*}
    \ev{C_{N}u, u} &\leq \ev{\qty(\qty(\frac{\beta^{2}(t)}{\gamma^{2}(t)} + k) I_{N} + Q_{N}^{-1})^{-1}u, u}_{H_{N}}, \text{ for all } u \in H_{N}\\
    \iff C_{N} &\preccurlyeq \qty(\qty(\frac{\beta^{2}(t)}{\gamma^{2}(t)} + k) I_{N} + Q_{N}^{-1})^{-1}.
  \end{align*}
  \paragraph{Step 5} Having established a Loewner order on \(C_{N}\), we now use this to place a bound on the operator norm of \(C_{N}\). Since \(C_{N}\) is positive semi-definite, the Loewner order translates directly into an ordering on operator norms:
  \[
    \onorm{C_{N}} \leq \onorm{\qty(\qty(\frac{\beta^{2}(t)}{\gamma^{2}(t)} + k) I_{N} + Q_{N}^{-1})^{-1}}.
  \] % TODO: add operator norm to nomenclature
  The spectrum of the operator \(\qty(\qty(\frac{\beta^{2}(t)}{\gamma^{2}(t)} + k) I_{N} + Q_{N}^{-1})^{-1}\) is given by the function \(\sigma(\lambda) = \frac{\lambda \gamma^{2}(t)}{\lambda (\beta^{2}(t) + k \gamma^{2}(t)) + \gamma^{2}(t)}\) evaluated over the spectrum of \(Q_{N}\). This function is monotone and increasing for \(\lambda \geq 0\), attaining its supremum at \(\frac{\gamma^{2}(t)}{\beta^{2}(t) + k \gamma^{2}(t)}\). Hence, we have
  \[
    \onorm{C_{N}} \leq \frac{\gamma^{2}(t)}{\beta^{2}(t) + k \gamma^{2}(t)}.
  \] Substituting this relationship in \Cref{eqn:dxmt},
  \begin{align*}
    \norm{D_{x} m_{1 \mid 0, t, N}(x_{0}, x)[h]}_{H_{C}} &\leq \frac{\beta(t)}{\gamma^{2}(t)} \onorm{C_{N}} \onorm{\Pi_{N}} \norm{h}_{H_{C}} \leq \frac{\beta(t)}{\beta^{2}(t) + k \gamma^{2}(t)} \norm{h}_{H_{C}}.
  \end{align*}
  It follows from the mean-value inequality \citep[][Theorem 2.1.19]{berger1977nonlinearity}, that for any \(x, y \in H\),
  \begin{align}
    \norm{m_{1 \mid 0, t, N}(x_{0}, x) - m_{1 \mid 0, t, N}(x_{0}, y)}_{H_{C}} &= \norm{m_{1 \mid 0, t, N}(x_{0}, x) - m_{1 \mid 0, t, N}(x_{0}, y)}_{H_{N}} \notag \\
    &\leq \frac{\beta(t)}{\beta^{2}(t) + k \gamma^{2}(t)} \norm{x - y}_{H_{C}}. \label{eqn:ineq}
  \end{align}%
  Passing \(N \to \infty\), the sequence of approximate posterior means \(m_{1 \mid 0, t, N}(x_{0}, x)\) converges to the true posterior mean \(m_{1 \mid 0, N}(x_{0}, x)\) (see Lemma \ref{lem:posteriormeanconvergence}). Since each approximation satisfies the inequality (\ref{eqn:ineq}) that is uniform in \(N\) and the norm is a continuous mapping, the true posterior mean \(m_{1 \mid 0, t}(x_{0}, x)\) also inherits the inequality.
  \[
    \norm{m_{1 \mid 0, t}(x_{0}, x) - m_{1 \mid 0, t}(x_{0}, y)}_{H_{C}} \leq \frac{\beta(t)}{\beta^{2}(t) + k \gamma^{2}(t)} \norm{x - y}_{H_{C}}.
  \]

  \paragraph{Step 7} We now substitute this relationship into the expression for the drift coefficient in \Cref{eqn:driftreexpressed}: a Lipschitz constant for the overall drift is the maximum of the Lipschitz constants for each term involving \(x_{t}\):
  \[
    \norm{f(t, x_{0}, x) - f(t, x_{0}, y)}_{H_{C}} \leq L(t) \norm{x - y}_{H_{C}},
  \]
  where
  \[
    L(t) =  \mathop{\underset{}{\max}}\qty{\abs{\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)}}, \abs{\dot{\beta}(t) - \beta\qty(\frac{\dot{\gamma}(t)}{\gamma(t)} - \frac{\varepsilon}{\gamma^{2}(t)})} \frac{\beta(t)}{\beta^{2}(t) + k \gamma^{2}(t)}}.
  \]
  This concludes the proof.%
  %
  %
  % TODO: add and cite brascamp-Lieb
  % TODO: explain why approximate posteriormeans converge
  %
  %Finally, note that at \(t = 0\), the posterior mean \(m_{1 \mid 0, t}(x_{0}, x)\) collapses to \(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0} = x_{0}]\) when \(x = x_{0}\) and is undefined when \(x \neq x_{0}\). We extend its definition at time \(t=0\) to equal \(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}]\) even when \(x_{0} \neq x\). With this extension, the Lipscchitz continuity holds also in the case where \(t = 0\) since it no longer changes with \(x\).
  %
  % TODO: Frechet derivative in nomenclature
  %
\end{proof}

\begin{lemma}\label{lem:frechetf}
  Let \(m_{1 \mid 0, t, N}(x_{0}, x)\) be an approximate posterior mean as defined in \Cref{eqn:apm}, with \(t \in(0,1)\) and \(N \geq 0\). Then the Frechet derivative of the mapping \(x \mapsto m_{1 \| 0, t, N}(x_{0}, x)\) in \(H_{C}\)-norm, in a direction \(h \in H_{C}\) is given by
  \[
    D_{x} m_{1 \mid 0, t, N}(x_{0}, x)[h] =\frac{\beta(t)}{\gamma^{2}(t)} \mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x)) \ev{\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x), h}_{H_{C}}]
  \]

  \begin{proof}
    We begin by taking the Frechet derivative of \(m_{1 \mid 0, t, N}(x_{0}, x)\) at \(x\) in a direction \(h \in H_{C}\). Applying the quotient rule (see \citealp[][Chapter 2.1]{berger1977nonlinearity}) and simplifying, we have
    \begin{align}
      D_{x} m_{1 \mid 0, t, N}(x_{0}, x)[h] &= D_{x}\qty{\frac{\int_{H_{N}} \xi_{1, N} \exp(-V_{1 \mid 0, t, N}(\xi_{1, N},x_{0}, x)) \mathbb{P}_{1 \mid 0, N}(\dd{\xi_{1, N}}, x_{0})}{ Z_{1 \mid 0, t, N}(x_{0}, x)}}[h] \notag \\
      &= \frac{1}{Z_{1 \mid 0, t, N}(x_{0}, x)} D_{x}U_{1 \mid 0, t, N}(x_{0}, x)[h] - m_{1 \mid 0, t, N}(x_{0}, x) \frac{D_{x} Z_{1 \mid 0, t, N}(x_{0}, x)[h]}{Z_{1 \mid 0, t, N}(x_{0}, x)}, \label{eqn:lem10fin}
    \end{align}
    where we define \(U_{1 \mid 0, t, N}(x _{0}, x) \coloneqq \int_{H_{N}} \xi_{1, N} \exp(-V_{1 \mid 0, t, N}(\xi_{1, N},x_{0}, x)) \mathbb{P}_{1 \mid 0, N}(\dd{\xi_{1, N}}, x_{0})\) to simplify notation. Evaluating the Frechet derivatives, we have
    \begin{align*}
      D_{x}U_{1 \mid 0, t, N}(x_{0}, x)[h] &= \frac{1}{\gamma^{2}(t)}\int_{H_{N}} \xi_{1, N} \ev{\alpha(t) \Pi_{N} x_{0} + \beta(t) \xi_{1, N} - x, h}_{H_{C}}\\[-0.5em]
      &\qquad \qquad \cdot \exp(-V_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x)) \mathbb{P}_{1 \mid 0, N}(\dd{\xi_{1, N}}, x_{0}), \\
      D_{x} Z_{1 \mid 0, t, N}(x_{0}, x)[h] &= \frac{1}{\gamma^{2}(t)}\int_{H_{N}} \ev{\alpha(t) \Pi_{N} x_{0} + \beta(t) \xi_{1, N} - x, h}_{H_{C}} \\[-0.5em]
      &\qquad \qquad \cdot \exp(-V_{1 \mid 0, t, N}(\xi_{1, N}, x_{0}, x)) \mathbb{P}_{1 \mid 0, N}(\dd{\xi_{1, N}}, x_{0}).
    \end{align*}
    Substituting these into \Cref{eqn:lem10fin} and recognising that the fractions come together to form the approximate posterior density, we have:
    \[
      D_{x} m_{1 \mid 0, t, N}(x_{0}, x)[h] = \frac{1}{\gamma^{2}(t)}\mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x))\ev{\alpha(t) \Pi_{N} x_{0} + \beta(t) \xi_{1, N} - x, h}_{H_{C}}].
    \]
    Adding and subtracting zero,
    \[
      0 = \frac{1}{\gamma^{2}(t)} \mathop{\mathbb{E}_{\mu_{1 \mid 0, t ,N}(\cdot, x_{0}, x)}}\qty[(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x)) \ev{- \alpha(t) \Pi_{N} x_{0}  + \beta(t) m_{1 \mid 0, t, N}(x_{0}, x) + x, h}_{H_{C}}],
    \]
    we arrive at the expression
    \[
      D_{x} m_{1 \mid 0, t, N}(x_{0}, x)[h] = \frac{\beta(t)}{\gamma^{2}(t)}\mathop{\mathbb{E}_{\mu_{1 \mid 0, t, N}(\cdot, x_{0}, x)}}\qty[(\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x))\ev{\xi_{1, N} - m_{1 \mid 0, t, N}(x_{0}, x), h}_{H_{C}}].
    \]
    This concludes the proof.
  \end{proof}
\end{lemma}

\begin{lemma}\label{lem:posteriormeanconvergence}
  For every \(x_{0}, x \in H\) and \(t \in (0, 1)\), the sequence of approximate posterior means \(\qty{m_{1 \mid 0, t, N}(x_{0}, x)}_{N=1}^{\infty}\)   as defined in \Cref{eqn:apm} converges to the true posterior mean \(m_{1 \mid 0, t}(x_{0}, x)\).

  \begin{proof}
    First, let us re-express the definition of \(m_{1 \mid 0, t, N}(x_{0}, x)\) by lifting the integrals into a common infinite-dimensional space:
    \begin{align}
      m_{1 \mid 0, t, N}( x_{0}, x) &= \int_{H_{C}} \Pi_{N} \xi_{1} \frac{1}{Z_{1 \mid 0, t, N}(x_{0}, x)} \exp(-V_{1 \mid 0, t}(\Pi_{N} \xi_{1}, \Pi_{N} x_{0}, x)) \mathbb{P}_{1 \mid 0}(\dd{\xi_{1}}, x_{0}),  \label{eqn:lift}\\
      \text{ where } Z_{1 \mid 0, t, N}(x_{0}, x) &= \int_{H_{C}} V_{1 \mid 0, t}(\Pi_{N} \xi_{1}, \Pi_{N} x_{0}, x) \mathbb{P}_{1 \mid 0}(\dd{\xi_{1}}, x_{0}).\notag
    \end{align}

    We define the sequence of functions \[f_{N}(\xi_{1}) \coloneqq \Pi_{N} \xi_{1} \frac{1}{Z_{1 \mid 0, t, N}(x_{0}, x)}\exp(- V_{1 \mid 0, t}(\Pi_{N} \xi_{1}, \Pi_{N} x_{0}, x)),\] and \[f(\xi_{1}) \coloneqq \xi \frac{1}{Z_{1 \mid 0, t}(x_{0}, x)}\exp(-V_{1 \mid 0, t}(\xi_{1}, x_{0}, x)),\] for fixed \(x_{0}\) and  \(x\). To show convergence, we appeal to the Vitali convergence theorem \citep{walnut2011vitali}, which is a generalisation of the dominated convergence theorem and states that if the sequence of functions \(f_{N}\) is pointwise-convergent to \(f\) and uniformly integrable, then the integral of the functions also converges to the integral of \(f\). We proceed in two steps: we first show pointwise convergence, and then show uniform integrability.

    \paragraph{Step 1: Pointwise Convergence} The numerator \(\Pi_{N} \xi_{1} \exp(-V_{1 \mid 0, t}(\Pi_{N} \xi_{1}, \Pi_{N} x_{0}, x))\) is clearly pointwise convergent to \(\xi_{1} \exp(-V_{1 \mid 0, t}(\xi_{1}, x_{0}, x))\) since for any fixed \(\xi_{1} \in H_{C}\), the projection \(\Pi_{N} \xi_{1}\) converges to \(\xi_{1}\) in \(H_{C}\)-norm, and \(V_{1 \mid 0, t, x}\) is continuous in all of its inputs. Hence, it remains to show convergence of the sequence of normalising constants \(Z_{1 \mid 0, t, N}(x_{0}, x)\).

    To this end, we apply the dominated convergence theorem to show that
    \[
      \lim\limits_{N \to \infty} \int_{H_{C}} \exp(-V_{1 \mid 0}(\Pi_{N} \xi_{1}, \Pi_{N} x_{0}, x)) \mathbb{P}_{1 \mid 0}(x_{0}) = \lim\limits_{N \to \infty} \int_{H_{C}} \exp(-V_{1 \mid 0}(\xi_{1}, x_{0}, x)) \mathbb{P}_{1 \mid 0}(\dd{\xi_{1}}, x_{0}).
    \]
    Since \(\Phi\) is strongly convex, it has a unique global minimum. This implies that the integrand on both sides are bounded by a constant \(M_{1} < \infty\) that does not depend on \(N\). Since the constant function is integrable on any probability space, it follows from the dominated convergence theorem that \(\lim\limits_{N \to \infty} Z_{1 \mid 0, t, N}(x_{0}, x) = Z_{1 \mid 0, t}(x_{0}, x)\).

    Finally, since the normalising constant is nonzero for any \(N\) and converges to a non-zero value, the functions \(f_{N}(\xi_{1})\) are pointwise convergent to \(f(\xi_{1})\).

    \paragraph{Step 2: Uniform Integrability}
    A sufficient condition for uniform integrability is that there exists a uniform bound on the expected squared norm of sequence of the functions \(f_{N}\) \citep[][Theorem 3.5]{billingsley2013convergence}:
    \begin{equation}
      \int_{H_{C}} \norm{\Pi_{N} \xi_{1}}_{H_{C}}^{2} \frac{1}{Z_{1 \mid 0, t, N}^{2}(x_{0}, x)} \exp(-2 V_{1 \mid 0, t}(\Pi_{N} \xi_{1}, \Pi_{N} x_{0}, x)) \mathbb{P}_{1 \mid0}(\dd{\xi_{1}}, x_{0}). \label{eqn:dc2}
    \end{equation}
    We will again employ the dominated convergence theorem to show that this sequence converges, and hence is bounded. First, pointwise convergence holds trivially since both the numerator and denominators converge, and the squared normalising factors \(Z^{2}_{1 \mid 0, t, N}(x_{0}, x)\) are positive for all \(N\) and converge to a positive value. Furthermore, the integrand is uniformly bounded by a constant \(\overline{M}\), since the strong convexity of \(\Phi\) ensures that the potential grows at least quadratically as \(\norm{\Pi_{N} \xi_{1}}_{H_{C}} \to \infty\) and hence overwhelms the quadratic growth of the \(\norm{\Pi_{N} \xi_{1}}^{2}_{H_{C}}\) pre-factor.

    The dominated convergence theroem therefore applies and it follows that the sequence of integrals in \Cref{eqn:dc2} is convergent and therefore bounded. Hence, the sequence of functions \(f_{N}\) is uniformly integrable.

    Since we have shown that the sequence of functions \(f_{N}\) is pointwise convergent and uniformly integrable, it follows that their integrals, which are equal to the approximate posterior means \(m_{1 \mid 0, t, N}(x_{0}, x)\), are convergent and converge to the true posterior mean \(m_{1 \mid 0, t}(x_{0}, x)\).
  \end{proof}
\end{lemma}

% \begin{lemma}\label{lem:aposteriormeancont}
%   Let \(\qty{m_{1 \mid 0, t, N}(x_{0}, x)}_{N=1}^{\infty}\)  be a sequence of approximate posterior means as defined in \Cref{eqn:apm}, with \(t \in (0, 1)\). For every \(N \geq 1\), the mapping \(x \mapsto m_{1 \mid 0, t, N}(x_{0}, x)\) is continuous in \(H_{C}\)-norm.

%   \begin{proof}
%     Let \(\qty{x_{j}}_{j=1}^{\infty}\) be a convergent sequence, converging to a vector \(x\) in \(H_{C}\)-norm. We apply the dominated convergence theorem to show that the corresponding sequence of approximate posterior means \(\qty{m_{1 \mid 0, t, N}(x_{0}, x_{j})}_{j=1}^{\infty}\) also converge in \(H_{C}\)-norm. Since the dominating arguments are identical to those in \Cref{lem:posteriormeanconvergence}, we provide only a sketch of a proof here.

%     We first lift the defining integrals for the approximate posterior means \(m_{1 \mid 0, t, N}(x_{0}, x)\) to integrals over a common infinite-dimensional space as in \Cref{lem:posteriormeanconvergence} (see Equation \ref{eqn:lift}). Since the potential \(V_{1 \mid 0, t}(\Pi_{N}, \xi_{1}, \Pi_{N}, x_{0}, x)\) is continuous in \(x\), an application of the dominated convergence theorem shows that the normaling constants converge as \(j \to \infty\). Hence, the integrands for the approximate posterior means converge pointwise. Again, the strong-convexity of \(\Phi\) implies a uniform bound on this integrand. A second application of the dominated convergence theorem hence implies convergence of the approximate posterior means: \(\lim\limits_{j \to \infty} m_{1 \mid 0, t, N}(x_{0}, x_{j}) = m_{1 \mid 0, t, N}(x_{0}, x)\). This concludes the proof.
%   \end{proof}
% \end{lemma}

% Since the arguments in the proof to \Cref{lem:aposteriormeancont} do not depend on the dimension of the orthogonal projection \(N\), they apply also to the true posterior mean. Hence, the true posterior mean is also continuous in the conditioning variable \(x_{t} =x\). We state this below as a corollary.

% \begin{corollary}
%   The true posterior mean \(m_{1 \mid 0, t}(x_{0}, x) = \mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0} = x_{0}, x_{t} = x]\) is continuous in \(x\), in \(H_{C}\)-norm.
% \end{corollary}

\section{Proof of \Cref{lem:manifold}}\label{prf:lem:manifold}

\restatelemmanifold*
\begin{proof}
  Our proof follows a similar overarching argument to to proof of \Cref{lem:bayes} in \Cref{prf:lem:bayes}: again, the expression \Cref{eqn:driftreexpressed} means it is sufficient to consider Lipschitz continuity of the mapping \(x\mapsto \mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0} = x_{0}, x_{t} = x]\). As before, we find a bound for the expression for the Frechet derivative of the posterior mean, expressed as a covariance. The assumption of bounded support in \(H_{C}\)-norm allows us to greatly simplify our arguments, meaning that we no longer require a Galerkin-type projection argument and directly provide our proof in infinite dimensions.

  % TODO: link/signpost to Galerkin in the main text when you sketch your proof to the bayes proposition

  As in \Cref{prf:lem:bayes}, we let \(\mu_{1 \mid 0, t}(\dd{\xi_{1}}, x_{0}, x)\) denote the posterior law of \(\xi_{1}\), conditional on \(\xi_{0} = x_{0}\) and \(x_{t} = x\). This time however, for each \(t \in (0, 1)\) we let the reference measure be \(\mathbb{P}_{t} \coloneqq \operatorname{N}(\alpha(t) \xi_{0}, \gamma^{2}(t) C)\). Note that the Cameron-Martin space of \(\gamma^{2}(t)C\) is identical to that of \(C\), equipped with an inner product scaled by \(\frac{1}{\gamma^{2}(t)}\). Since \(\beta(t) \xi_{1}\) is almost-surely in \(H_{C}\), and hence also the Cameron-Martin space of \(\gamma^{2}(t)C\), \(H_{\gamma^{2}(t)C}\), the measure \(\mu_{1 \mid 0, t}(\dd{\xi_{1}}, x_{0}, x)\) is absolutely continuous with respect to \(\mathbb{P}_{t}\):
  \begin{align*}
    \dv{\mu_{1 \mid 0, t}(\cdot, x_{0}, x)}{\mathbb{P}_{t}}{}(\xi_{1}) &= \frac{1}{Z_{1 \mid 0, t}(x_{0}, x)}\exp(-V_{1 \mid 0, t}(\xi_{1}, x_{0}, x)), \\
    \text{ where } V_{1 \mid 0, t}(\xi_{1}, x_{0}, x) &= \frac{1}{\gamma^{2}(t)} \norm{\alpha(t) x_{0} + \beta(t) \xi_{1} - x}_{H_{C}}^{2},
  \end{align*}
  and \(Z_{1 \mid 0, t}(x_{0}, x) \coloneqq \int_{H_{C}} \exp(-V_{1 \mid 0, t}(\xi_{1}, x_{0}, x)) \mathbb{P}_{t}(\dd{\xi_{1}})\) is a normalising constant. We define \(m_{t}(x_{0}, x)\) as the posterior mean:
  \[
    m_{t}(x_{0}, x) \coloneqq \mathop{\mathbb{E}_{\mu_{1 \mid 0, t}(\cdot, x_{0}, x)}}\qty[\xi_{1}] = \int_{H_{C}} \xi_{1} \mu_{1 \mid 0, t}(\dd{\xi_{1}}, x_{0}, x).
  \]
  Following an approach analogous to that given in the proof to \Cref{lem:frechetf}, we take the Frechet derivative in the direction \(h \in H_{C}\) and again arrive at a covariance:
  \[
    D_{x} m_{t}(x_{0}, x)[h] = \frac{\beta(t)}{\gamma^{2}(t)} \mathop{\mathbb{E}_{\mu_{1 \mid 0, t}(\cdot, x_{0}, x)}}\qty[ (\xi_{1} - m_{t}(x_{0}, x))\ev{\xi_{1} - m_{t}(x_{0}, x), h}_{H_{C}}]
  \]
  Taking the norm in \(H_{C}\) and applying the Cauchy-Schwarz inequality, we have
  \[
    \norm{D_{x} m_{t}(x_{0}, x)[h]}_{H_{C}} \leq \frac{\beta(t)}{\gamma^{2}(t)} \mathop{\mathbb{E}_{\mu_{1 \mid 0, t}(\cdot, x_{0}, x)}}\qty[\norm{\xi_{1} - m_{t}(x_{0}, x)}_{H_{C}}^{2}] \norm{h}_{H_{C}}
  \]
  Using the fact that \(0 \leq \mathop{\mathbb{E}}\qty[\norm{\xi_{1} - m_{t}(x_{0}, x)}_{H_{C}}^{2}] = \mathop{\mathbb{E}}\qty[\norm{\xi_{1}}_{H_{C}}^{2}] - \norm{m_{t}(x_{0}, x)}^{2}\) and \(\norm{\xi_{1}^{2}}_{H_{C}} \leq R^{2}\) almost surely, we conclude
  \[
    \norm{D_{x} m_{t}(x_{0}, x)[h]}_{H_{C}} \leq \frac{R^{2} \beta(t)}{\gamma^{2}(t)} \norm{h}_{H_{C}}.
  \]
  Finally, we apply the mean-value inequality \citep[][Theorem 2.1.19]{berger1977nonlinearity} and conclude that \(m_{t}(x_{0}, x)\) is Lipschitz in \(H_{C}\)-norm with Lipschitz constant at most \(\frac{R^{2}\beta(t)}{\gamma^{2}(t)}\):
  \[
    \norm{m_{t}(x_{0}, x) - m_{t}(x_{0}, y)}_{H_{C}} \leq \frac{R^{2}\beta(t)}{\gamma^{2}(t)} \norm{x - y}_{H_{C}}.
  \]
  Substituting this into \Cref{eqn:driftreexpressed} gives the Lipschitz constant for the overall mapping \(x \mapsto f(t, x_{0}, x)\). This concludes the proof.
\end{proof}

\section{Proof of \Cref{thm:exist}} \label{prf:thm:exist}
\restatethmexist*
\begin{proof}
  We begin by addressing the behavior of the drift and its associated Lipschitz constant, \(L(t)\), at the initial time \(t = 0\). The drift coefficient \(f(t, x_{0}, x)\) is defined via conditional expectations of the stochastic interpolant \(x_{t}\), conditioned on the events \(\xi_{0} = x_{0}\) and \(x_{t} = x\).

  At the specific instant \(t = 0\), the conditioning event \(x_{t} = x\) becomes \(\xi_{0} = x\) by definition of the stochastic interpolant. For this event to be consistent with the condition \(\xi_{0} = x_{0}\), we must have \(x = x_{0}\). Consequently at time \(0\), the drift \(f(0, x_{0}, x)\) is only well-defined where \(x_{0} = x\). This is satisfied by the initial condition \(X_{0} = \xi_{0}\) for the CB-SDE (\ref{eqn:cbsde}).

  However, the Lipschitz condition is a statement about the behavior of the drift under perturbations, i.e., comparing \(f(t, x_0, x)\) and \(f(t, x_{0}, y)\) for \(x \neq y\). Since the drift is not defined for such perturbations at \(t = 0\), the Lipschitz condition is only meaningful for \(t > 0\).

  Therefore, for the purpose the arguments below, we  extend the function \(L(t)\) to be continuous on the entire closed interval \([0, \overline{t}]\). Without loss of generality, we define \(L(0) := \lim_{t \to 0^{+}} L(t)\). This is justified because the value of the drift at a single point in time does not affect the SDE's solution.
  \paragraph{Step 1: Partitioning of time domain}

  With this definition, we can proceed with the proof assuming that \(L(t)\) is continuous and therefore bounded on the compact interval \([0, \overline{t}]\). Hence, it is possible to create a finite partition \(0 = \tau_{0} < \tau_{1} < \tau_{2} < \cdots < \tau_{k} < \cdots < \tau_{K} = {\overline{t}}\) of \([0, {\overline{t}}]\) with \(K < \infty\) such that
  \[
    q_{k} \coloneqq (\tau_{k} - \tau_{k-1}) \sup_{t \in [\tau_{k-1}, \tau_{k}]} {L}(t) < 1, \quad \text{ for all } k = 1, \ldots, K.
  \]

  \paragraph{Step 2: Existence of Strong Solutions}

  For each \(k = 1, \ldots, K\), consider the Banach space \(B_{k}\) of all continuous, \(H_{C}\)-valued functions on \([\tau_{k-1}, \tau_{k}]\) equipped with the following norm:
  \[\norm{Y}_{B_{k}} \coloneqq \sup_{t \in [\tau_{k-1}, \tau_{k}]} \norm{Y(t)}_{H_{C}}.\]
  To argue existence of a strong solution to the CB-SDE on \([0, {\overline{t}}]\), we will apply Banach's fixed point theorem inductively and piecewise on the intervals \(\qty[\tau_{k-1}, \tau_{k}]\) and pathwise for all events \(\omega\) in the sample space \(\Omega\), to build a solution \({X}_{t}\) on \([0, \overline{t}]\).
  % TODO: Xhat is unique on [0, tbar] so X must be unique on [0, thetainverse(tbar)]

  Fix any event \(\omega \in \Omega\), so that \(\xi_{0}(\omega)\) and \({W}_{t}(\omega)\) are respectively the outcomes of the random variable \(\xi_{0}\) and the Wiener process at time \(t\), and define \({X}_{0}(\omega) \coloneqq \xi_{0}(\omega)\). Furthermore, let
  \[
    \widetilde{W}_{k, t} \coloneqq \int_{\tau_{k-1}}^{t} \sqrt{2\varepsilon }\dd{ {W}_{s}}.
  \] We proceed by induction: for each \(k = 1, \ldots, K\), having defined \({X}_{\tau_{k-1}}(\omega)\), we define the mapping \(\Psi_{k, \omega} : B_{k} \to B_{k}\) as follows. For any \(Y \in B_{k}\),
  \begin{equation}
    (\Psi_{k, \omega} Y)(t) = \int_{\tau_{k-1}}^{t} {f}\qty(s, \xi_{0}(\omega), {X}_{\tau_{k-1}}(\omega) + \widetilde{W}_{k, s}(\omega) + Y(s)) \dd{s}, \quad \text{ for all } t \in [\tau_{k-1}, \tau_{k}]. \label{eqn:banach-iteration}
  \end{equation}

  For any \(Y, Y' \in B_{k}\), we have
  \begin{align*}
    &\norm{\Psi_{k, \omega} Y - \Psi_{k, \omega} Y'}_{B_{k}} = \sup_{t \in [\tau_{k-1}, \tau]} \norm{ (\Psi_{k, \omega} Y - \Psi_{k, \omega} Y')(t) }_{H_{C}} \\
    &\qquad\leq \int_{\tau_{k-1}}^{\tau_{k}} \norm{ {f}\qty(s, \xi_{0}(\omega), {X}_{\tau_{k-1}}(\omega) + \widetilde{W}_{k, s}(\omega) + Y(s)) - {f}\qty(s, \xi_{0}(\omega), {X}_{\tau_{k-1}}(\omega) + \widetilde{W}_{k, s}(\omega) + Y'(s))}_{H_{C}} \dd{s}\\
    &\qquad\leq (\tau_{k} - \tau_{k-1})\sup_{t \in [\tau_{k-1}, \tau]} \qty[{L}(t) \norm{Y(t) - Y'(t)}_{H_{C}}] \\
    &\qquad\leq (\tau_{k} - \tau_{k-1})\sup_{t \in [\tau_{k-1}, \tau]} {L}(t) \sup_{t \in [\tau_{k-1}, \tau]} \norm{Y(t) - Y'(t)}_{H_{C}} \\
    &\qquad= q_{k} \norm{Y - Y'}_{B_{k}},
  \end{align*}
  where \(q_{k} < 1\) by construction of the interval. By Banach's fixed point theorem, it follows that there exists a unique \(Y^{*} \in B_{k}\) such that \(\Psi_{k, \omega} Y^{*} = Y^{*}\).

  For every \(t \in [\tau_{k-1}, \tau_{k}]\), we let \({X}_{t}(\omega) \coloneqq {X}_{\tau_{k-1}}(\omega) +  \widetilde{W}_{k, t}(\omega) + Y^{*}(t)\) for all \(t \in [\tau_{k-1}, \tau_{k}]\). Substituting this definition into the fixed point identity \(\Psi_{k, \omega} Y^{*} = Y^{*}\), we have
  \begin{align*}
    {X}_{t}(\omega) - {X}_{\tau_{k-1}}(\omega) - \widetilde{W}_{k, t}(\omega) &= \int_{\tau_{k-1}}^{t} {f}(s, \xi_{0}(\omega), {X}_{s}(\omega)) \dd{s} \\
    \implies {X}_{t}(\omega) &= {X}_{\tau_{k-1}}(\omega) +  \int_{\tau_{k-1}}^{\tau_{k}} {f}(s, \xi_{0}(\omega) {X}_{s}(\omega)) \dd{s} + \int_{\tau_{k-1}}^{t} \sqrt{2\varepsilon } \dd{ {W}(\omega)},
  \end{align*}
  which is the integral form of the CB-SDE (\ref{eqn:cbsde}), expressed pathwise with the chosen probability event \(\omega \in \Omega\) and defined on the interval \(t \in [\tau_{k-1}, \tau_{k}]\).

  Since \(\omega\) was chosen arbitrarily, we may repeat this process for every \(\omega \in \Omega\) to build a stochatic process \({X}_{t}\) on the interval \(t \in [\tau_{k-1}, \tau_{k}]\). Now that we have a definition of \({X}_{\tau_{k}}(\omega)\), we may repeat the inductive step for \(k \leftarrow k+1\). This builds a stochatic process \({X}_{t}\) on the entire desired interval \(t \in [ 0, {\overline{t}}]\).

  It remains to check that \({X}_{t}\) is \({\mathbb{F}}\)-adapted on \([0, {\overline{t}}]\). Again, employing induction, we may observe that \(X_{0} = \xi_{0}\) is by definition \(\mathcal{F}_{0}\)-measurable. Then, for each \(k = 1, \ldots, K\), we are given that \(X_{\tau_{k-1}}\) is \(\mathcal{F}_{ \tau_{k-1}}\)-measurable. We can view every contraction-mapping iteration as if it were applied for all \(\omega \in \Omega\) simultaneously. Suppose the initial guesses \(Y_{\omega} \in B_{k}\) are such that \(Y_{\omega}(t)\) is \(\mathcal{F}_{t}\)-measurable as a function of \(\omega\), for all \(t \in [\tau_{k-1}, \tau_{k}]\). Each application of the contraction mapping, \((\Psi_{k,\omega}(Y_{\omega}))(t)\), is also \(\mathcal{F}_{t}\)-measurable as a function of \(\omega\), since the integrand in \Cref{eqn:banach-iteration} is the composition of a continuous function with a \(\mathcal{F}_{t}\)-measurable function. Hence, every time we perform a Banach iteration, the outcome at time \(t \in [\tau_{k-1}, \tau]\) is \(\mathcal{F}_{t}\)-measurable. Since \(\sigma\)-fields are closed under countable pointwise limits, it follows that \(Y^{*}_{\omega}(t)\) and thus \({X}_{t}(\omega)\) are \(\mathcal{F}_{t}\)-measurable for all \(t \in [\tau_{k-1}, \tau_{k}]\). Repeating the induction for all steps up to \(k = K\) ensures that \({X}_{t}(\omega)\) is \(\mathcal{F}^{*}_{t}\)-measurable for all \(t \in [0, {\overline{t}}]\) and hence \({X}_{t}\) is \({\mathbb{F}}\)-adapted on \([0, {\overline{t}}]\). This concludes the proof.
\end{proof}

\section{Proof of \Cref{thm:uniq}} \label{prf:thm:uniq}
\restatethmuniq*

\begin{proof}
  As in the proof in \Cref{prf:thm:exist} for the existence of strong solutions, we assume without loss of generality that \(L(t)\) is continuous on \([0, \overline{t}]\). Let \({X}_{t}\) and \({X}_{t}'\) be two strong solutions for the same initial condition, \({X}_{0} = {X}_{0}' = \xi_{0}\) and driven by the same Wiener process \({W}_{t}\) on \([0, {\overline{t}}]\). \textit{A priori}, it is not guaranteed that \(\norm{ {X}_{t} - {X}_{t}'}_{H_{C}} < \infty\) since \({X}_{t} - {X}_{t}'\) may not be in \(H_{C}\). However, for each \(N \geq 1\), it is guaranteed that the projected diference \(P_{N}( {X}_{t} - {X}_{t}') \in H_{C}\) since the range of \(P_{N}\) is by definition a subspace of \(H_{C}\) due to \(C\) being a positive-definite operator. It therefore holds that
  \begin{align*}
    \dv{t}P_{N}\qty({X}_{t} - {X}_{t}') &= P_{N}\qty({f}(t, \xi_{0}, {X}_{t}) - {f}(t, \xi_{0}, {X}_{t}')) \\
    \implies \dv{t} \norm{ P_{N} \qty({X}_{t} - {X}_{t}')}_{H_{C}} &\leq \norm{P_{N}\qty({f}(t, \xi_{0}, {X}_{t}) - {f}(t, \xi_{0}, {X}_{t}'))}_{H_{C}} \\
    &\leq {L}(t)\norm{P_{N}\qty( {X}_{t} - {X}_{t}')}_{H_{C}},
  \end{align*}
  where \({L}(t)\) is as defined in \Cref{eqn:defLhatt}.

  We now apply Groenwall's inequality \citep[][Theorem 1.2.2]{ames1997inequalities} to the quantity \(\norm{P_{N}\qty({X}_{t} - {X}_{t}')}_{H_{C}}\) as a function of \(t\): since \({L}(t)\) is real-valued and continuous on \([0, {\overline{t}}]\), we have:
  \[
    \norm{P_{N}\qty( {X}_{t} - {X}_{t}')}_{H_{C}} \leq \norm{ P_{N}\qty( {X}_{0} - {X}_{0}')}_{H_{C}} \exp(\int_{0}^{ {\overline{t}}} {L}(t) \norm{P_{N}\qty( {X}_{t} - {X}_{t}')}_{H_{C}} ).
  \]

  Since by definition \({X}_{0} = {X}_{0}' = \xi_{0}\), so \({X}_{0} - {X}_{0}' = 0\), it follows that
  \[
    \norm{P_{N}\qty({X}_{t} - {X}_{t}')}_{H_{C}} = 0,
  \]
  for all \(t \in [0, {\overline{t}}]\). Since this equality is true for every \(N \geq 1\), we pass \(N \to \infty\). It follows that \(\norm{ {X}_{t} - {X}_{t}'}_{H_{C}} = 0\) and therefore
  \[
    {X}_{t} = {X}_{t}', \text{ for all } t \in [0, \overline{t}].
  \]
  This concludes the proof.
\end{proof}

\section{Proof \Cref{lem:ni}}\label{prf:lem:ni}
\restatelemni*
% In this section, we argue formally why the coefficients in the alternative parameterisation (Equation \ref{eqn:driftbadparam}) are not integrable on \([0,1]\). We make use of the following lemma.

\begin{proof}
  We consider the function \(y(t) \coloneqq \gamma^{2}(t)\), which satisfies \(y(0) =y(1) = 0\) and \(\dot{y}(t) = 2 \dot{\gamma}(t) \gamma(t)\). The function \(c(t)\) can be re-written in terms of \(y(t)\) as
  \[
    c(t) = \frac{ \dot{y}(t) - 2\varepsilon}{2y(t)}.
  \]
  Consider the improper integral
  \begin{align*}
    I_{-} &= \lim\limits_{a \to 0^{+}} \int_{a}^{\frac{1}{2}} c(t) \dd{t} \\
    &= \frac{1}{2} \log y\qty(\frac{1}{2}) - \lim\limits_{a \to 0^{+}} \qty[\frac{1}{2} \log y(a) + \varepsilon \int_{a}^{\frac{1}{2}} \frac{1}{y(t)} \dd{t}]
  \end{align*}
  A necessary condition for \(I_{-}\) to converge to a finite number is that \(\varepsilon > 0\). In this case, it is necessary that
  \[
    -1 = \lim\limits_{a \to 0^{+}} \frac{\log y(a)}{2 \varepsilon \int_{a}^{\frac{1}{2}} \frac{1}{y(t)} \dd{t}} = \lim\limits_{a \to 0^{+}} -\frac{\dot{y}(a)}{ 2 \varepsilon} \iff \dot{y}(0) = 2\varepsilon.
  \]
  A similar analysis for the integral \(I_{+} \coloneqq \lim\limits_{a \to 1^{-}} \int_{\frac{1}{2}}^{a} c(t)\) shows that it is necessary that \(\dot{y}(1) = -2\varepsilon\).

  Taken together, these conditions imply that there exists some function \(h(t)\) differentiable on \((0, 1)\) satisfying the boundary conditions \(h(0) = 2\varepsilon\) and \(h(1) = -2\varepsilon\), such that
  \[
    y(t) =  t(1-t) h(t).
  \]
  Substituting this into the definition of \(c(t)\), we have
  \[
    I_{+} = \int_{\frac{1}{2}}^{1} c(t)\dd{t}= \int_{\frac{1}{2}}^{1} \frac{(1 - 2t) h(t) - 2\varepsilon}{2t(1-t) h(t)}\dd{t}.
  \]
  In the limit as \(t \to 1^{-}\), the integrand converges to \(-\infty\) and satisfies the following limit:
  \[
    \lim\limits_{t \to 1^{-}}  \frac{-\qty[\frac{(1-2t) h(t) - 2\varepsilon}{2t(1-t) h(t)}]}{\frac{1}{1-t}} = 1.
  \]
  Hence, the integral \(I_{+}\) converges if and only if the integral \(\int_{\frac{1}{2}}^{1} \frac{1}{1-t} \dd{t}\) converges. Since this integral does not converge, we conclude that \(I_{+}\) does not converge and hence \(c(t)\) is not integrable for any permissible choice of \(\gamma\).
\end{proof}

We have established that the function \(c(t)\) is not integrable on \([0, 1]\). We can now argue that the coefficient \(\dot{\beta}(t) - \beta(t) c(t)\) on the expectation \(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}, x_{t}]\) in the alternative parameterisation (Equation \ref{eqn:driftbadparam}) is not integrable on \([0, 1]\). The singularity  as \(t \to 1^{-}\), which we have shown is of the order \(\frac{1}{1-t}\), is not avoided in the coefficient \(\dot{\beta}(t) - \beta(t)c(t)\). This is because \(\beta(1) = 1\) by definition, and \(\dot{\beta}\) is bounded on \([0, 1]\) due to the continuous differentiability of \(\beta\). Hence, the coefficient \(\dot{\beta}(t) - \beta(t)c(t)\) has a singularity of order \(\frac{1}{1-t}\) as \(t\to 1^{-}\) and is not integrable on \([0, 1]\).

\section{Proof of \Cref{prp:loss}}\label{prf:prp:loss}
\restateprploss*
\begin{proof}
  We first consider the difference between the denoising matching objectives \(\mathrm{TDM}_{t}(\widetilde{\eta})\) and \(\mathrm{PDM}_{t}(\widetilde{\eta})\) when \(\mathrm{TDM}_{t}(\widetilde{\eta})\) is finite. The result for the velocity matching objectives follows via analogous arguments.
  \paragraph{Step 1: \(\mathrm{PDM}_{t}(\widetilde{\eta}) - \mathrm{TDM}_{t}(\widetilde{\eta})\)}

  Let \(\qty{e_{n}}_{n=1}^{\infty}\) be an eigenbasis of \(U\), let \(U_{N}\) be the linear span of \(\qty{e_{1}, \ldots, e_{N}}\), and denote by \(\Pi_{N}\) the orthogonal projection operator from \(U\) into \(U_{N}\). We perform these projections to ensure that all terms we work with are finite when manipulating the expectations. For any \(N \geq 1\), we have:
  \begin{align}
    &\mathop{\mathbb{E}}\qty[ \norm{\Pi_{N}\qty(\widetilde{\eta}(t, \xi_{0}, x_{t}) - z)}_{U}^{2} ] \label{eqn:lhs}\\
    &\qquad= \mathop{\mathbb{E}}\qty[ \norm{\Pi_{N}\big(\qty(\widetilde{\eta}(t, \xi_{0}, x_{t}) - \eta(t, \xi_{0}, x_{t})) - (\eta(t, \xi_{0}, x_{t}) - z)\big)}_{U}^{2} ] \notag\\
    &\qquad= \mathop{\mathbb{E}}\qty[\norm{\Pi_{N}( \widetilde{\eta}(t, \xi_{0}, x_{t}) - \eta(t, \xi_{0}, x_{t}))}_{U}^{2}] + \mathop{\mathbb{E}}\qty[\norm{\Pi_{N} \big(\eta(t, \xi_{0}, x_{t}) - z\big)}_{U}^{2}]\notag\\[-0.25em]
    &\qquad\mathrel{\phantom{=}}\,+ 2 \mathop{\mathbb{E}}\qty[ \ev{ \Pi_{N}\big(\widetilde{\eta}(t, \xi_{0}, x_{t}) - \eta(t, \xi_{0}, x)\big), \Pi_{N} \big(\eta(t, \xi_{0}, x_{t}) - z\big)} ] \notag\\
    &\qquad= \mathop{\mathbb{E}}\qty[\norm{\Pi_{N}( \widetilde{\eta}(t, \xi_{0}, x_{t}) - \eta(t, \xi_{0}, x_{t}))}_{U}^{2}] + \mathop{\mathbb{E}}\qty[\norm{\Pi_{N} \big(\eta(t, \xi_{0}, x_{t}) - z\big)}_{U}^{2}], \label{eqn:lossn}
  \end{align}
  where the final equality is due to an application of the law of iterated expectations which holds from the linearity of the inner product and projection operator.

  We now take the limit as \(N \to \infty\). The first term in \Cref{eqn:lossn} converges to \(\mathrm{TDM}_{t}(\widetilde{\eta})\) which by assumption is finite. To analyse the second term, we consider the cases \(U = H\) and \(U = H_{C}\).

  In the case where \(U = H\), we have
  \[
    \lim\limits_{n \to \infty} \mathop{\mathbb{E}}\qty[\norm{\Pi_{N} \eta(t, \xi_{0}, x_{t}) - z}^{2}_{H}] = \mathop{\mathbb{E}}\qty[\norm{\mathop{\mathbb{E}}\qty[z \mid \xi_{0}, x_{t}] - z}_{H}^{2}] < \infty,
  \]
  since the Gaussian \(z \sim \operatorname{N}(0, C)\) has finite second moment in \(H\)-norm due to the covariance operator \(C\) being trace-class. Therefore in the limit as \(N \to \infty\), the left-hand side (Equation \ref{eqn:lhs}) is finite and converges to \(\mathrm{PDM}_{t}(\widetilde{\eta})\).

  In the case where \(U = H_{C}\), we use the fact that \(\eta(t,\xi_{0}, x_{t}) = \frac{\beta(t)}{\gamma(t)}\qty(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}, x_{t}] - \xi_{1})\). If \(\xi_{1}\) is supported on the subspace \(H_{C}\) and has finite second moment, then
  \[
    \lim\limits_{n \to \infty} \mathop{\mathbb{E}}\qty[\norm{\Pi_{N} \eta(t, \xi_{0}, x_{t}) - z}^{2}_{H}]  = \frac{\beta^{2}(t)}{\gamma^{2}(t)} \mathop{\mathbb{E}}\qty[\norm{\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}, x_{t}] - \xi_{1}}_{H_{C}}^{2}] < \infty.
  \]
  Hence in both cases,  \(\mathrm{TDM}_{t}(\widetilde{\eta})\) and \(\mathrm{PDM}_{t}(\widetilde{\eta})\) differ only by a finite constant.

  \paragraph{Step 2: \(\mathrm{PVM}_{t}(\widetilde{\eta}) - \mathrm{TVM}_{t}(\widetilde{\eta})\)}

  We now consider the difference between \(\mathrm{TVM}_{t}(\widetilde{\varphi})\) and \(\mathrm{PVM}_{t}(\widetilde{\varphi})\) when \(\mathrm{TVM}_{t}(\widetilde{\varphi})\) is finite. Following similar analysis to the above, we have
  \begin{align*}
    \mathop{\mathbb{E}}\qty[\norm{\Pi_{N}( \widetilde{\varphi}(t, \xi_{0}, x_{t}) - (\dot{\alpha}(t) \xi_{0} + \dot{\beta}_{t} \xi_{1}) )}^{2}_{U}] &= \dot{\beta}^{2}(t)\mathop{\mathbb{E}}\qty[\norm{\Pi_{N} \big(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}, x_{t}] - \xi_{1}\big)}_{U}^{2}] \\
    &\mathrel{\phantom{=}}\, + \mathop{\mathbb{E}}\qty[\norm{\Pi_{N}(\widetilde{\varphi}(t, \xi_{0}, x_{t}) - \varphi(t, \xi_{0}, x_{t}))}^{2}_{U}].
  \end{align*}
  In the limit as \(N \to \infty\), the second term converges to \(\mathrm{TVM}_{t}(\widetilde{\varphi})\). In the case where \(U = H_{C}\), if \(\xi_{1}\) is supported on \(H_{C}\) with finite second moment, then the first term also converges, and hence the left-hand side converges to \(\mathrm{PVM}_{t}(\widetilde{\varphi})\), is finite, and differs from \(\mathrm{TVM}_{t}(\widetilde{\varphi})\) only by a constant.

  In the case where \(U = H_{C}\), we use the fact that \(\mathop{\mathbb{E}}\qty[\xi_{1} \mid \xi_{0}, x_{t}] - \xi_{1} = \frac{\gamma(t)}{\beta(t)}\qty(\mathop{\mathbb{E}}\qty[z \mid \xi_{0}, x_{t}] - z)\) and hence the first term converges to
  \[
    \frac{\dot{\beta}^{2}(t) \gamma^{2}(t)}{\beta^{2}(t)} \mathop{\mathbb{E}}\qty[\norm{\mathop{\mathbb{E}}\qty[z \mid \xi_{0}, x_{t}] - z}^{2}_{H}],
  \]
  which is finite since \(C\) is trace-class. Again, the left-hand side converges to \(\mathrm{PVM}_{t}(\widetilde{\varphi})\), is finite, and differs from \(\mathrm{TVM}_{t}(\widetilde{\varphi})\) only by a constant. This concludes the proof.
\end{proof}

\section{Proof of \Cref{lem:tc}} \label{prf:lem:tc}
\restatelemtc*
\begin{proof}
  Let \(h(t) \coloneqq \varepsilon/\gamma(t)\) and \(C \coloneqq \int_0^1 h(\sigma) d\sigma\), which is finite and positive by assumption. We define a new time variable \(s(t)\) by
  \[ s(t) \coloneqq \frac{1}{C} \int_{0}^{t} h(\sigma) \dd{\sigma}. \]
  Since \(h(t) > 0\) for \(t \in (0, 1)\), \(s(t)\) is a strictly increasing, continuously differentiable bijection from \([0, 1]\) to itself. We define the time-change \(\theta(t)\) as its inverse, \(\theta(t) \coloneqq s^{-1}(t)\). This is also strictly increasing and continuously differentiable on \((0, 1)\), with derivative \(
  \dot{\theta}(t) = \frac{C}{\varepsilon} \gamma(\theta(t))\).
  Substituting this into the definition of \(\hat{c}(t)\), we have
  \[
    \hat{c}(t) = \frac{C}{\varepsilon} \qty(\dot{\gamma}(\theta(t)) \gamma(\theta(t))) - C.
  \]
  By assumption, the function \(\dot{\gamma}(t)\gamma(t)\) has a continuous extension to \([0, 1]\). Since \(\theta(t)\) is also continuous on \([0, 1]\), their composition \(\dot{\gamma}(\theta(t))\gamma(\theta(t))\) is continuous on \([0, 1]\). Therefore, the final expression for \(\hat{c}(t)\) is continuous on \([0, 1]\). This implies that \(\hat{c}(t)\), initially defined only on \((0, 1)\), has well-defined finite limits as \(t \to 0^{+}\) and \(t \to 1^{-}\), and thus admits a continuous extension to the compact interval \([0, 1]\).
\end{proof}

\section{Proof of \Cref{thm:w2}}\label{prf:thm:w2}
\restatetheoremw*
\begin{proof}
  Let \(\widetilde{X}_{t}\) be the solution to the CB-SDE when using the approximate velocity \(\widetilde{\varphi}\) and denoiser \(\widetilde{\eta}\) to form the approximate drift \(\widetilde{f}(t, x_{0}, x) \coloneqq \widetilde{\varphi}(t, x_{0}, x) + c(t) \widetilde{\eta}(t, x_{0}, x)\). From \Cref{thm:cbsde}, we know that the law of \(X_{t}\) is equal to \(\mu_{t \mid 0}(\dd{x_{t}}, \xi_{0})\). Hence,  we couple \(\widetilde{X}_{t}\) with \(X_{t}\) via the same \(C\)-Wiener process \(W_{t}\) and analyse the expected squared distance between these processes.

  We consider the TC-CB-SDE, which has a unique solution \(\hat{X}_{t} = X_{\theta(t)}\) on the interval \([0, \theta^{-1}(\overline{t})]\) when driven by the Wiener process \(\hat{W}_{t} = W_{\theta(t)}\). Let \(\hat{\widetilde{X}}_{t} = \widetilde{X}_{\theta(t)}\) be the time-changed approximate counterpart. Applying Ito's lemma (\citealp[Theorem 4.2]{da2014stochastic}) to \(E(t) \coloneqq \mathop{\mathbb{E}}\qty[\norm{ \hat{\widetilde{X}}_{t} - \hat{X}_{t}}^{2}_{H}]\), we have
  \[
    \dv{E(t)}{t} = 2 \mathop{\mathbb{E}}\qty[\ev{ \hat{\widetilde{X}}_{t} - \hat{X}_{t}, \widetilde{f}(\theta(t), \xi_{0}, \hat{\widetilde{X}}_{t}) - f(\theta(t), \xi_{0}, \hat{X}_{t}) }_{H} \dot{\theta}(t)].
  \]
  We add \(0 = -\widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t}) + \widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t})\) to the second argument of the inner product to split this into two terms:
  \begin{align}
    \dv{E(t)}{t} &= \overbrace{2 \mathop{\mathbb{E}}\qty[\ev{ \hat{\widetilde{X}}_{t} - \hat{X}_{t}, \widetilde{f}(\theta(t), \xi_{0}, \hat{\widetilde{X}}_{t}) -\widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t})}_{H} \dot{\theta}(t)]}^{\text{propagation error term}} \notag \\
    &\phantom{=}\, + \underbrace{2 \mathop{\mathbb{E}}\qty[ \ev{ \hat{\widetilde{X}}_{t} - \hat{X}_{t}, \widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t})  - f(\theta(t), \xi_{0}, \hat{X}_{t})}_{H} \dot{\theta}(t)]}_{\text{training error term}}. \label{eqn:b}
  \end{align}
  We place a bound on each term using the Cauchy-Schwarz inequality. For the propagation error term, we make use of the fact that \(\widetilde{\varphi}\) and \(\widetilde{\eta}\) are Lipschitz continuous, so that for each \(t \in [0, \theta^{-1}(\overline{t})]\) and \(x_{0} \in H\), the mapping
  \[
    x \mapsto \widetilde{f}(\theta(t), x_{0}, x) \dot{\theta}(t) = \widetilde{\varphi}(\theta(t), x_{0}, x)\dot{\theta}(t) + \hat{c}(t) \widetilde{\eta}(\theta(t), x_{0}, x)
  \]
  is Lipschitz continuous in \(H\)-norm with Lipschitz constant \(L(t) = \mathop{\underset{}{\max}}\qty{\dot{\theta}(t), \hat{c}(t)} \widetilde{L}\). Since by \Cref{lem:tc}, both \(\hat{c}\) and \(\dot{\theta}\) are continuous on the compact interval \([0, \theta^{-1}(\overline{t})]\), the uniform bound \(\overline{c} \coloneqq \mathop{\underset{t \in [0, \theta^{-1}(\overline{t})]}{\max}}\qty(\dot{\theta}(t) + \abs{ \hat{c}(t)})\) is finite. It follows that
  \begin{equation}
    2 \mathop{\mathbb{E}}\qty[ \ev{ \hat{\widetilde{X}}_{t} - \hat{X}_{t}, \widetilde{f}(\theta(t), \xi_{0}, \hat{\widetilde{X}}_{t}) -\widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t})}_{H} \dot{\theta}(t) ]\leq 2 \overline{c} \widetilde{L} E(t). \label{eqn:petb}
  \end{equation}
  For the training error term, we have
  \begin{align}
    &2 \mathop{\mathbb{E}}\qty[\ev{ \hat{\widetilde{X}}_{t} - \hat{X}_{t}, \widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t})  - f(\theta(t), \xi_{0}, \hat{X}_{t})}_{H} \dot{\theta}(t)] \notag \\
    &\qquad \leq 2 \mathop{\mathbb{E}}\qty[\norm{ \hat{\widetilde{X}}_{t} - \hat{X}_{t}}_{H} \norm{ \qty(\widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t}) - f(\theta(t), \xi_{0}, \hat{X}_{t})) \dot{\theta}(t)}_{H}]\notag \\
    &\qquad \leq E(t) + \mathop{\mathbb{E}}\qty[\norm{ \widetilde{f}(\theta(t), \xi_{0}, \hat{X}_{t}) - f(\theta(t), \xi_{0}, \hat{X}_{t}) \dot{\theta}(t)}^{2}_{H}] \notag \\
    &\qquad \leq E(t) + 2 \dot{\theta}^{2}(t) \mathrm{TVM}_{\theta(t)}( \widetilde{\varphi}) + 2 \hat{c}^{2}(t) \mathrm{TDM}_{\theta(t)}(\widetilde{\eta}) \notag \\
    &\qquad \leq E(t) + 2 \overline{c}^{2}\qty( \mathrm{TVM}_{\theta(t)}(\widetilde{\varphi}) + \mathrm{TDM}_{\theta(t)}(\widetilde{\eta})) \label{eqn:tetb}
  \end{align}
  Substituting our bounds on the propagation error term (Equation \ref{eqn:petb}) and training error term (Equation \ref{eqn:tetb}) into \Cref{eqn:b}, we have
  \[
    \dv{E(t)}{t}  \leq (2\overline{c} \widetilde{L} + 1) E(t) + 2\overline{c}^{2} \qty( \mathrm{TVM}_{\theta(t)}(  \widetilde{\varphi}) + \mathrm{TDM}_{\theta(t)} ( \widetilde{\eta})).
  \]
  Applying Groenwall's inequality to \(E(t)\) on the interval \([0, \theta^{-1}(\overline{t})]\), we have
  \begin{align*}
    \mathcal{W}^{2}_{2}(\overline{t}) &\leq \mathop{\mathbb{E}}\qty[ \norm{ \overline{\widetilde{X}_{t}} - \overline{X_{t}} }^{2}_{H} ] \\
    & = E(\theta^{-1}(\overline{t})) \leq 2 \overline{c}^{2} \int_{0}^{ \theta^{-1}(\overline{t})} \qty( \mathrm{TVM}_{\theta(t)}( \widetilde{\varphi} ) + \mathrm{TDM}_{\theta(t)} (\widetilde{\eta}) )e^{(2  \overline{c} \widetilde{L} + 1 )(\theta^{-1}(\overline{t}) - t) }\dd{t} \\
    &\leq 2\overline{c}^{2} e^{2 \overline{c} \widetilde{L} + 1} \int_{0}^{\theta^{-1}(\overline{t})} \mathrm{TVM}_{\theta(t)}(\widetilde{\varphi}) + \mathrm{TDM}_{\theta(t)}(\widetilde{\eta})\dd{t}
  \end{align*}
  This concludes the proof.
\end{proof}

\section{Proof of \Cref{lem:thetaconditions}}\label{prf:lem:thetaconditions}
\restatelemthetaconditions*
\begin{proof}
  We have:
  \begin{equation}
    \hat{c}(t) = \frac{b - 2\varepsilon}{2 \sqrt{b \theta(t)(1-\theta(t))}} \dot{\theta}(t) - \sqrt{\frac{b \theta(t)}{1-\theta(t)}} \dot{\theta}(t) \label{eqn:chatme}
  \end{equation}
  By inspection, \(\hat{c}(t)\)  is finite on all \((0, 1)\) since \(\theta(t) \in (0, 1)\) and \(\dot{\theta}(t)\) is continuous. We analyse the limits at the endpoints by considering each case in turn.

  First, when \(\varepsilon = \frac{b}{2}\), the first term  of \Cref{eqn:chatme} vanishes, reducing analysis to the second term. This is finite on \([0, 1)\), so we need only consider the limit  \(t  \to 1^{-}\). This is finite if and only if \(\lim\limits_{t \to 1^{-}} q(t)\) is finite, where \(q(t) \coloneqq \frac{\dot{\theta}(t)}{\sqrt{1-\theta(t)}}\). Using a substitution \(y(t) = \sqrt{1-\theta(t)}\), we have \(q(t) = -2 \dot{y}(t)\) and hence
  \begin{align*}
    \lim\limits_{t \to 1^{-}} q(t) < \infty &\iff \lim\limits_{t \to 1^{-}} -\dot{y}(t) = \lim\limits_{t \to 1^{-}} \frac{y(t)}{1 - t} < \infty\\
    &\iff \lim\limits_{t \to 1^{-}} \frac{y^{2}(t)}{(1-t)^{2}}  = \lim\limits_{t \to 1^{-}} \frac{1-\theta(t)}{(1-t)^{2}} = \lim\limits_{t \to 1^{-}}  \frac{\dot{\theta}(t)}{2(1-t)} < \infty.
  \end{align*}
  Therefore, when \(\varepsilon = \frac{b}{2}\), \(\hat{c}(t)\) has a finite continuous extension on \([0, 1]\) if and only if condition (\ref{lem:thetaconditions:1}) in \Cref{lem:thetaconditions} holds.

  Now we consider the case \(\varepsilon \neq \frac{b}{2}\). We now additionally require the first term in \Cref{eqn:chatme} to have finite limits at the endpoints. In the limit \(t \to 1^{-}\), the first term is finite if and only if  \(\lim\limits_{t \to 1^{-}} \frac{\dot{\theta}(t)}{\sqrt{1-\theta(t)}} \) is finite, which is the same condition as discussed above. It remains to check the limit \(t \to 0^{+}\). Here, the first term is finite if and only if \(\lim\limits_{t \to 0^{+}} r(t) < \infty\), where \(r(t) \coloneqq \frac{\dot{\theta}(t)}{\sqrt{\theta(t)}}\). Considering a substitution \(u(t) \coloneqq \sqrt{\theta(t)}\), we have \(r(t) = 2 \dot{u}(t)\) and hence
  \begin{align*}
    \lim\limits_{t \to 0^{+}}  r(t) < \infty &\iff \lim\limits_{t \to 0^{+}}  \dot{u}(t) = \lim\limits_{t \to 0^{+}}  \frac{u(t)}{t} < \infty \\
    &\iff \lim\limits_{t \to 0^{+}}  \frac{u^{2}(t)}{t^{2}} = \lim\limits_{t \to 0^{+}} \frac{\theta(t)}{t^{2}} = \lim\limits_{t \to 0^{+}} \frac{\dot{\theta}(t)}{2t} < \infty.
  \end{align*}
  Hence, when \(\varepsilon \neq \frac{b}{2}\), \(\hat{c}(t)\) has a finite continuous extension on \([0, 1]\) if and only if both conditions (\ref{lem:thetaconditions:1}) and (\ref{lem:thetaconditions:2}) in \Cref{lem:thetaconditions} hold. This concludes the proof.
\end{proof}